---
title: "CSCI E-63C Week 13 Problem Set"
author: 'Joshua Sacher'
date: '`r Sys.Date()`'

output:
  html_document:
    df_print: kable
    number_sections: true
    toc: true
    toc_float:
      collapsed: false
---

```{r setup, include=FALSE}
ptStart <- proc.time()
library(neuralnet)
library(ggplot2)
library(GGally)
library(grid)
library(scales)
library(scatterplot3d)
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 8)
```

```{r, echo=FALSE}
library(grid)
plot.nn <-
  function (x,
            rep = NULL,
            x.entry = NULL,
            x.out = NULL,
            radius = 0.15,
            arrow.length = 0.2,
            intercept = TRUE,
            intercept.factor = 0.4,
            information = TRUE,
            information.pos = 0.1,
            col.entry.synapse = "black",
            col.entry = "black",
            col.hidden = "black",
            col.hidden.synapse = "black",
            col.out = "black",
            col.out.synapse = "black",
            col.intercept = "blue",
            fontsize = 12,
            dimension = 6,
            show.weights = TRUE,
            file = NULL,
            ...)
  {
    net <- x
    if (is.null(net$weights))
      stop("weights were not calculated")
    if (!is.null(file) && !is.character(file))
      stop("'file' must be a string")
    if (is.null(rep)) {
      for (i in 1:length(net$weights)) {
        if (!is.null(file))
          file.rep <- paste(file, ".", i, sep = "")
        else
          file.rep <- NULL
        #dev.new()
        plot.nn(
          net,
          rep = i,
          x.entry,
          x.out,
          radius,
          arrow.length,
          intercept,
          intercept.factor,
          information,
          information.pos,
          col.entry.synapse,
          col.entry,
          col.hidden,
          col.hidden.synapse,
          col.out,
          col.out.synapse,
          col.intercept,
          fontsize,
          dimension,
          show.weights,
          file.rep,
          ...
        )
      }
    }
    else {
      if (is.character(file) && file.exists(file))
        stop(sprintf("%s already exists", sQuote(file)))
      result.matrix <- t(net$result.matrix)
      if (rep == "best")
        rep <- as.integer(which.min(result.matrix[, "error"]))
      if (rep > length(net$weights))
        stop("'rep' does not exist")
      weights <- net$weights[[rep]]
      if (is.null(x.entry))
        x.entry <- 0.5 - (arrow.length / 2) * length(weights)
      if (is.null(x.out))
        x.out <- 0.5 + (arrow.length / 2) * length(weights)
      width <- max(x.out - x.entry + 0.2, 0.8) * 8
      radius <- radius / dimension
      entry.label <- net$model.list$variables
      out.label <- net$model.list$response
      neuron.count <- array(0, length(weights) + 1)
      neuron.count[1] <- nrow(weights[[1]]) - 1
      neuron.count[2] <- ncol(weights[[1]])
      x.position <- array(0, length(weights) + 1)
      x.position[1] <- x.entry
      x.position[length(weights) + 1] <- x.out
      if (length(weights) > 1)
        for (i in 2:length(weights)) {
          neuron.count[i + 1] <- ncol(weights[[i]])
          x.position[i] <- x.entry + (i - 1) * (x.out -
                                                  x.entry) / length(weights)
        }
      y.step <- 1 / (neuron.count + 1)
      y.position <- array(0, length(weights) + 1)
      y.intercept <- 1 - 2 * radius
      information.pos <- min(min(y.step) - 0.1, 0.2)
      if (length(entry.label) != neuron.count[1]) {
        if (length(entry.label) < neuron.count[1]) {
          tmp <- NULL
          for (i in 1:(neuron.count[1] - length(entry.label))) {
            tmp <- c(tmp, "no name")
          }
          entry.label <- c(entry.label, tmp)
        }
      }
      if (length(out.label) != neuron.count[length(neuron.count)]) {
        if (length(out.label) < neuron.count[length(neuron.count)]) {
          tmp <- NULL
          for (i in 1:(neuron.count[length(neuron.count)] -
                       length(out.label))) {
            tmp <- c(tmp, "no name")
          }
          out.label <- c(out.label, tmp)
        }
      }
      grid.newpage()
      pushViewport(viewport(
        name = "plot.area",
        width = unit(dimension,
                     "inches"),
        height = unit(dimension, "inches")
      ))
      for (k in 1:length(weights)) {
        for (i in 1:neuron.count[k]) {
          y.position[k] <- y.position[k] + y.step[k]
          y.tmp <- 0
          for (j in 1:neuron.count[k + 1]) {
            y.tmp <- y.tmp + y.step[k + 1]
            result <- calculate.delta(c(x.position[k],
                                        x.position[k + 1]),
                                      c(y.position[k], y.tmp),
                                      radius)
            x <-
              c(x.position[k], x.position[k + 1] - result[1])
            y <- c(y.position[k], y.tmp + result[2])
            grid.lines(
              x = x,
              y = y,
              arrow = arrow(length = unit(0.15,
                                          "cm"), type = "closed"),
              gp = gpar(fill = col.hidden.synapse,
                        col = col.hidden.synapse, ...)
            )
            if (show.weights)
              draw.text(
                label = weights[[k]][neuron.count[k] -
                                       i + 2, neuron.count[k + 1] - j + 1],
                x = c(x.position[k],
                      x.position[k + 1]),
                y = c(y.position[k],
                      y.tmp),
                xy.null = 1.25 * result,
                color = col.hidden.synapse,
                fontsize = fontsize - 2,
                ...
              )
          }
          if (k == 1) {
            grid.lines(
              x = c((x.position[1] - arrow.length),
                    x.position[1] - radius),
              y = y.position[k],
              arrow = arrow(length = unit(0.15, "cm"),
                            type = "closed"),
              gp = gpar(fill = col.entry.synapse,
                        col = col.entry.synapse, ...)
            )
            draw.text(
              label = entry.label[(neuron.count[1] +
                                     1) - i],
              x = c((x.position - arrow.length),
                    x.position[1] - radius),
              y = c(y.position[k],
                    y.position[k]),
              xy.null = c(0, 0),
              color = col.entry.synapse,
              fontsize = fontsize,
              ...
            )
            grid.circle(
              x = x.position[k],
              y = y.position[k],
              r = radius,
              gp = gpar(fill = "white", col = col.entry,
                        ...)
            )
          }
          else {
            grid.circle(
              x = x.position[k],
              y = y.position[k],
              r = radius,
              gp = gpar(fill = "white", col = col.hidden,
                        ...)
            )
          }
        }
      }
      out <- length(neuron.count)
      for (i in 1:neuron.count[out]) {
        y.position[out] <- y.position[out] + y.step[out]
        grid.lines(
          x = c(x.position[out] + radius, x.position[out] +
                  arrow.length),
          y = y.position[out],
          arrow = arrow(length = unit(0.15,
                                      "cm"), type = "closed"),
          gp = gpar(fill = col.out.synapse,
                    col = col.out.synapse, ...)
        )
        draw.text(
          label = out.label[(neuron.count[out] +
                               1) - i],
          x = c((x.position[out] + radius), x.position[out] +
                  arrow.length),
          y = c(y.position[out], y.position[out]),
          xy.null = c(0, 0),
          color = col.out.synapse,
          fontsize = fontsize,
          ...
        )
        grid.circle(
          x = x.position[out],
          y = y.position[out],
          r = radius,
          gp = gpar(fill = "white", col = col.out,
                    ...)
        )
      }
      if (intercept) {
        for (k in 1:length(weights)) {
          y.tmp <- 0
          x.intercept <-
            (x.position[k + 1] - x.position[k]) *
            intercept.factor + x.position[k]
          for (i in 1:neuron.count[k + 1]) {
            y.tmp <- y.tmp + y.step[k + 1]
            result <-
              calculate.delta(c(x.intercept, x.position[k +
                                                          1]),
                              c(y.intercept, y.tmp),
                              radius)
            x <- c(x.intercept, x.position[k + 1] - result[1])
            y <- c(y.intercept, y.tmp + result[2])
            grid.lines(
              x = x,
              y = y,
              arrow = arrow(length = unit(0.15,
                                          "cm"), type = "closed"),
              gp = gpar(fill = col.intercept,
                        col = col.intercept, ...)
            )
            xy.null <-
              cbind(x.position[k + 1] - x.intercept -
                      2 * result[1],
                    -(y.tmp - y.intercept + 2 *
                        result[2]))
            if (show.weights)
              draw.text(
                label = weights[[k]][1, neuron.count[k +
                                                       1] - i + 1],
                x = c(x.intercept, x.position[k +
                                                1]),
                y = c(y.intercept, y.tmp),
                xy.null = xy.null,
                color = col.intercept,
                alignment = c("right",
                              "bottom"),
                fontsize = fontsize - 2,
                ...
              )
          }
          grid.circle(
            x = x.intercept,
            y = y.intercept,
            r = radius,
            gp = gpar(fill = "white", col = col.intercept,
                      ...)
          )
          grid.text(
            1,
            x = x.intercept,
            y = y.intercept,
            gp = gpar(col = col.intercept, ...)
          )
        }
      }
      if (information)
        grid.text(
          paste(
            "Error: ",
            round(result.matrix[rep,
                                "error"], 6),
            "   Steps: ",
            result.matrix[rep,
                          "steps"],
            sep = ""
          ),
          x = 0.5,
          y = information.pos,
          just = "bottom",
          gp = gpar(fontsize = fontsize +
                      2, ...)
        )
      popViewport()
      if (!is.null(file)) {
        weight.plot <- recordPlot()
        save(weight.plot, file = file)
      }
    }
  }
calculate.delta <-
  function (x, y, r)
  {
    delta.x <- x[2] - x[1]
    delta.y <- y[2] - y[1]
    x.null <- r / sqrt(delta.x ^ 2 + delta.y ^ 2) * delta.x
    if (y[1] < y[2])
      y.null <- -sqrt(r ^ 2 - x.null ^ 2)
    else if (y[1] > y[2])
      y.null <- sqrt(r ^ 2 - x.null ^ 2)
    else
      y.null <- 0
    c(x.null, y.null)
  }
draw.text <-
  function (label,
            x,
            y,
            xy.null = c(0, 0),
            color,
            alignment = c("left",
                          "bottom"),
            ...)
  {
    x.label <- x[1] + xy.null[1]
    y.label <- y[1] - xy.null[2]
    x.delta <- x[2] - x[1]
    y.delta <- y[2] - y[1]
    angle = atan(y.delta / x.delta) * (180 / pi)
    if (angle < 0)
      angle <- angle + 0
    else if (angle > 0)
      angle <- angle - 0
    if (is.numeric(label))
      label <- round(label, 5)
    pushViewport(
      viewport(
        x = x.label,
        y = y.label,
        width = 0,
        height = ,
        angle = angle,
        name = "vp1",
        just = alignment
      )
    )
    grid.text(
      label,
      x = 0,
      y = unit(0.75, "mm"),
      just = alignment,
      gp = gpar(col = color, ...)
    )
    popViewport()
  }
```

# Preface

The goal of this problem set is to develop some intuition about the impact of the number of nodes in the hidden layer of the neural network.  We will use few simulated examples to have clear understanding of the structure of the data we are modeling and will assess how performance of the neural network model is impacted by the structure in the data and the setup of the network.

First of all, to compensate for lack of coverage on this topic in ISLR, let's go over a couple of simple examples.  We start with simulating a simple two class dataset in 2D predictor space with an outcome representative of an interaction between attributes.  (Please notice that for the problems you will be working on this week you will be asked below to simulate a dataset using a different model.)

```{r, fig.height=7,fig.width=7}
# fix seed so that narrative always matches the plots:
set.seed(1234567890)
nObs <- 1000
ctrPos <- 2
xyTmp <- matrix(rnorm(4 * nObs), ncol = 2)
xyCtrsTmp <-
  matrix(sample(c(-1, 1) * ctrPos, nObs * 4, replace = TRUE), ncol = 2)
xyTmp <- xyTmp + xyCtrsTmp
gTmp <- paste0("class", (1 + sign(apply(xyCtrsTmp, 1, prod))) / 2)
plot(
  xyTmp,
  col = as.numeric(factor(gTmp)),
  pch = as.numeric(factor(gTmp)),
  xlab = "X1",
  ylab = "X2"
)
abline(h = 0)
abline(v = 0)
```

Symbol color and shape indicate class.  Typical problem that will present a problem for any approach estimating a single linear decision boundary.  We used similar simulated data for the random forest (week 10) problem set.

## One hidden node

We can fit simple neural network (using all default values in the call to `neuralnet` -- notice that both covariates and outcome have to be numeric as opposed to factor) and plot its layout (allowing for its output to be included in Rmarkdown generated report actually seems to be quite painful - one has to overwrite original implementation of `plot.nn` with the one that doesn't call `dev.new()` that is included in this Rmarkdown file with `echo=FALSE` -- to do the same you have to include that block into your Rmarkdown file also):

```{r,fig.height=7,fig.width=7}
### Doesn't run: "requires numeric/complex ... arguments"
### nnRes <- neuralnet(g~X1+X2,data.frame(g=gTmp,xyTmp))
nnRes <- neuralnet(g~X1+X2,data.frame(g=as.numeric(factor(gTmp)),xyTmp))
plot(nnRes)
```

That shows us a model with one node in a single hidden layer (default parameters).

We can lookup actual model predictions and recalculate them from input variables (in the field `covariate`) and model weight and activation function (fields `weights` and `act.fct` respectively):

```{r}
head(nnRes$net.result[[1]])
cbind(rep(1,6),nnRes$act.fct(cbind(rep(1,6),nnRes$covariate[1:6,])%*%nnRes$weights[[1]][[1]]))%*%nnRes$weights[[1]][[2]]
```

Notice that input parameter `linear.output` governs whether activation function is called on the value of the output node or not:

```{r}
nnResNLO <- neuralnet(g~X1+X2,data.frame(g=as.numeric(factor(gTmp)),xyTmp),linear.output=FALSE)
head(nnResNLO$net.result[[1]])
nnResNLO$act.fct(cbind(rep(1,6),nnResNLO$act.fct(cbind(rep(1,6),nnResNLO$covariate[1:6,])%*%nnResNLO$weights[[1]][[1]]))%*%nnResNLO$weights[[1]][[2]])
quantile(nnResNLO$net.result[[1]])
```

As the last statement (the quantiles of the predicted out) above shows, the use of activation function limiting predicted values to $[0;1]$ range when modeling outcome taking values outside of $[0;1]$ interval does not result in a very useful model. In this case with true outcome values constrained to $\{1,2\}$ so that the error is minimized by predicting every outcome to be as close to $1$ as possible.

Using binary -- 0 or 1 -- outcome produces more useful model when activation function is applied to the output node (`linear.output=FALSE`) and allows use of cross-entropy error function (often used in classification setting in combination with the activation function applied to the output layer):

```{r}
nnResNLO01 <-
  neuralnet(g ~ X1 + X2, data.frame(g = as.numeric(factor(gTmp)) - 1, xyTmp), linear.output =
              FALSE)
quantile(nnResNLO01$net.result[[1]], c(0, 0.1, 0.25, 0.5, 1))
if (FALSE) {
  # from v.3.5.x(?) this started to throw an error instead of silent switch to "sse":
  nnResNLO12CE <-
    neuralnet(
      g ~ X1 + X2,
      data.frame(g = as.numeric(factor(gTmp)), xyTmp),
      linear.output = FALSE,
      err.fct = "ce"
    )
}
nnResNLO01CE <-
  neuralnet(
    g ~ X1 + X2,
    data.frame(g = as.numeric(factor(gTmp)) - 1, xyTmp),
    linear.output = FALSE,
    err.fct = "ce"
  )
head(nnResNLO01CE$net.result[[1]])
nnResNLO01CE$act.fct(cbind(
  rep(1, 6),
  nnResNLO01CE$act.fct(
    cbind(rep(1, 6), nnResNLO01CE$covariate[1:6, ]) %*% nnResNLO01CE$weights[[1]][[1]]
  )
) %*% nnResNLO01CE$weights[[1]][[2]])
quantile(nnResNLO01CE$net.result[[1]])
```

We can plot model output indicating class identity (left panel below) that tells us that when true outcome values are constrained to 1 or 2, sum of squared errors is used as error function and output node values are used as-is (not transformed by activation function -- `linear.output=TRUE` by default), the majority of the points were estimated to be close to 1 or 1.6 and that majority of those estimated to be close to 1 correspond to the about half of the observations at the first level of the class factor (i.e. numerical value of 1).  It is also easy to see that those with predicted value of 1.6 represent roughly 1:2 mix of observations from the first and second levels of the outcome respectively, so that $1.6 \approx (1+2*2)/3 = 5/3$ approximately equals average of their numerical values corresponding to the levels of the factor representing them. 

The nature of the model estimated by `neuralnet` in this (very simple!) case becomes even more intuitive if we render all points in the area encompassing our training set with model predictions and overlay training dataset on top of that (right panel below).  It is immediately apparent that this model identified a line in this 2D space separating one cloud of points belonging mostly to one class from all others so that predicted values are approximately equal to the average outcome on each side of this decision boundary.

```{r,fig.height=6,fig.width=12}
plotNNpreds2D2class <-
  function(inpNN,
           inpClassThresh,
           inpGrid = (-60:60) / 10) {
    tmpClrPch <- as.numeric(factor(inpNN$response))
    plot(inpNN$net.result[[1]], col = tmpClrPch, pch = tmpClrPch)
    table(inpNN$net.result[[1]][, 1] > inpClassThresh, inpNN$response)
    xyGridTmp <-
      cbind(X1 = rep(inpGrid, length(inpGrid)), X2 = sort(rep(inpGrid, length(inpGrid))))
    # before predict.nn existed: 
    #gridValsTmp <- compute(inpNN,xyGridTmp)$net.result
    gridValsTmp <- predict(inpNN, xyGridTmp)
    errTmp <-
      sum(inpNN$err.fct(inpNN$net.result[[1]][, 1], inpNN$response))
    plot(
      xyGridTmp,
      col = as.numeric(gridValsTmp > inpClassThresh) + 1,
      pch = 20,
      cex = 0.3,
      main = paste("Error:", round(errTmp, 6))
    )
    points(inpNN$covariate, col = tmpClrPch, pch = tmpClrPch)
    ## Equations defining decision boundary:
    ## 1*w0 + X1*w1 + X2*w2 = 0, i.e.:
    ## 0 = inpNN$weights[[1]][1]+inpNN$weights[[1]][2]*X1+inpNN$weights[[1]][3]*X2, i.e:
    ## X2 = (-inpNN$weights[[1]][1] - inpNN$weights[[1]][2]*X1) / inpNN$weights[[1]][3]
    for (iTmp in 1:ncol(inpNN$weights[[1]][[1]])) {
      abline(
        -inpNN$weights[[1]][[1]][1, iTmp] / inpNN$weights[[1]][[1]][3, iTmp],
        -inpNN$weights[[1]][[1]][2, iTmp] / inpNN$weights[[1]][[1]][3, iTmp],
        lty = 2,
        lwd = 2
      )
    }
  }
old.par <- par(mfrow = c(1, 2), ps = 16)
plotNNpreds2D2class(nnRes, 1.3)
par(old.par)
```

Similar, aside from a different position of decision boundary, results can be obtained when using binary representation of the outcome with cross-entropy as error function together with applying activation function to the output layer:

```{r,fig.width=8,fig.height=8}
plot(nnResNLO01CE)
```

Once activation function is applied to the output node, the output values are bound to the $[0,1]$ interval.  The predicted values that are far enough from the decision boundary are also approximately set to the average of the outcome in that subspace:

```{r,fig.height=6,fig.width=12}
old.par <- par(mfrow = c(1, 2), ps = 16)
plotNNpreds2D2class(nnResNLO01CE, 0.5)
par(old.par)
```

The important points resulting from the results shown in the figures above are the following:

* as simple of a model as the one that was employed here (with one node in a single hidden layer along with all other default parameters) cannot do much better than what we observed here
* because calling (default - logistic) activation function on a given linear combination of the input variables more or less amounts to assigning almost all points on one side of hyperplane (line in 2D, plane in 3D, etc.) to zero and on the other side -- to unity
* weights involved in transforming outcome of the hidden layer into model predictions will change those zeroes and ones to values closer to the desired outcome values, but still, use of such a simple model (with a single hidden node) employed here to prime our intuition more or less amounts to splitting covariate space into two half spaces by a hyperplane and assigning almost constant outcomes to the vast majority of the points on either side of it
* the weights for the inputs into the single hidden node shown in the network layout plot above and stored in `weights` field of the result returned by `neuralnet` define this hyperplane (line in 2D, etc.) shown as dashes in the panels on the right above
* this hyperplane is where sum of weighted input variables and an intercept is identical zero (and thus the result of logistic activation function is 0.5 rapidly becoming zero or one for points further away from this boundary)

## Two hidden nodes, single hidden layer

Now, let's add another node to the hidden layer of this network.  From the above, we know what to expect as a result of that -- another hyperplane (line in 2D) will be added to the space of covariates now dividing it into (depending on whether those hyperplans are almost parallel or not) three or four subspaces, consequently, assigning most of the points to three or four potentially different constants.  Clearly, this level of granularity could suffice for developing a model that would do quite well in our toy example.

To have more than one node in the hidden layer we set `hidden` parameter to the number of nodes in it (length of vector provided as `hidden` parameter governs the number of the hidden *layers* in the network -- we still use one layer here):

```{r,fig.height=7,fig.width=7}
set.seed(1234567)
nnRes2 <-
  neuralnet(g ~ X1 + X2, data.frame(g = as.numeric(factor(gTmp)), xyTmp), hidden = 2)
plot(nnRes2)
```

We can see that now resulting network has two nodes in a single hidden layer, which two covariates enter with weights that are approximately comparable in magnitude and opposite in sign.  Their comparably weighted sum added to a constant close to one gives the outcome value of this model. The effect of those weights in defining decision boundaries in the space of predictors is best seen from the figure below:

```{r,fig.height=6,fig.width=12}
old.par <- par(mfrow = c(1, 2), ps = 16)
plotNNpreds2D2class(nnRes2, 1.5)
par(old.par)
```

This model sets up two almost parallel lines that encompass most of the observations from the first class, leaving most of the observations from the second class outside of the resulting slab.  Now let's repeat fitting neural network three times (each time starting with random choice of starting weights in the model) and compare stability of the resulting models:

```{r,fig.height=6,fig.width=9}
old.par <- par(mfcol = c(2, 3), ps = 16)
for (iTry in 1:3) {
  nnRes2 <-
    neuralnet(g ~ X1 + X2, data.frame(g = as.numeric(factor(gTmp)), xyTmp), hidden =
                2)
  plotNNpreds2D2class(nnRes2, 1.5)
}
par(old.par)
```

We can see that quite frequently given the parameters used the process converges to a suboptimal solution with about half of the observations remaining in "gray" zone where their assignment to either of the classes is not immediately apparent.

Aside from the multitude of local minima for neural network fitting procedure that could prevent it from finding better solutions, the main point to take from this exercise is that adding more nodes to the hidden layer (with all other default choices employed here) amounts to adding more hyperplanes bisecting the space of predictors, creating more and more subspaces where the outcome can take different values (often close to a constant in each subspace).  Obviously, the geometry of the resulting decision surfaces can become quite complicated even with modest number of nodes in the hidden layer. Lastly, these considerations provide some intuition for considering what could be a useful number of hidden nodes in the model. In thinking about that it might be useful to consider how many such hyperplanes could be sufficient to effectively separate observations belonging to different outcome categories.  Not that we necessarily would have such knowledge ahead of time, but this might prove to be a complementary way to think about the problem in addition to the often sited empirical guidelines that are based on the number of predictor variables, etc.

The point of this problem set is to assess how these aspects of neural network fitting play out in another simulated dataset.

Lastly, we also would like you to consider what combination of error function and output node transformation you would like to use for this week problem set.  Below are three calls of `neuralnet` timed using different combinations of `err.fct` and `linear.output`:

```{r}
system.time(invisible(
  neuralnet(
    g ~ X1 + X2,
    data.frame(g = as.numeric(factor(gTmp)) - 1, xyTmp),
    hidden = 2,
    linear.output = TRUE,
    err.fct = "sse"
  )
))
system.time(invisible(
  neuralnet(
    g ~ X1 + X2,
    data.frame(g = as.numeric(factor(gTmp)) - 1, xyTmp),
    hidden = 2,
    linear.output = FALSE,
    err.fct = "sse"
  )
))
system.time(invisible(
  neuralnet(
    g ~ X1 + X2,
    data.frame(g = as.numeric(factor(gTmp)) - 1, xyTmp),
    hidden = 2,
    linear.output = FALSE,
    err.fct = "ce"
  )
))
```

Obviously, for our toy example use of untransformed outcome from the output node and sum of squares as error function results in the fastest convergence.  But then the decision as to how to translate its predictions to class categories is yours.  On the other hand, one might argue that more principled approach since we are dealing with classification problem would be to use logistic transform of the outcome to contain it within $[0;1]$ interval and cross-entropy as an error function.  Except that it seems to run nearly two orders of magnitude slower in this case and does not necessarily converge with default values for the rest of the arguments.  Please feel free to experiment with how they will perform for the problems you are presented with below and decide for yourself which combination of these parameters is more suitable for the task at hand.

## Expand formula code

From lecture notes

```{r expand formula function}
expand.formula <- function(f, data = NULL) {
  # convert formula into a plain string, f.str becomes something like # "Y~X1+X2" or "Y~.":
  f.str <- deparse(f)
  # check if we are dealing with a shortcut formula Y~. (we simply
  # check if the last character in the formula string is “.”). If we are,
  # we also verify that the dataframe to take variables from is actually
  # provided; if the formula is not a shortcut, then this function has nothing 
  # left to do and returns the original formula immediately:
  if (grepl("\\.$", f.str)[1]) {
    if (is.null(data)) {
      stop("Shortcut formula ~. requires a dataframe")
    }
  } else {
    return(f)
  }
  # if we got to this point (i.e. we did not execute the return statement
  # above), we are dealing with the shortcut formula ~. and need to expand:
  # replace everything starting with any spaces followed by '~' through
  # the end with an empty string, this leaves us with the dependent
  # variable name; e.g. if the formula string was "Y~X",
  # dependent.name will be now set to "Y" (you can further modify this code to
  # deal with multi-class formulas such as "Y1+Y2+Y3~."):
  dependent.name <- sub("\\s*~.*", "", f.str)
  # substitute using regular expression 
  n <- colnames(data)
  # get the names of all variables (columns) in the ‘data’
  # remove the dependent variable name from the list of variables provided
  # with the data object (if it is not there, the following code has no effect):
  n <- n[n != dependent.name]
  # we want to use ALL the remaining variables in the formula,
  # this is what Y~. means! Make a string of “name1 + name2 + ...” :
  rhs = paste(n, collapse = " + ")
  # now substitute “.” In “Y~.” with the rhs name1+name2+...
  f.str <- sub("\\.$", rhs, f.str)
  # convert modified string back to formula object, make sure we bind
  # the new formula to the right environment:
  f <- as.formula(f.str, env = environment(f))
  return(f)
}

neuralnet.fx <- function(f, data, ...) {
  # in the function definition line we ask for the two first arguments
  # to be assigned to the formal variables ‘f’ and ‘data’, while ‘...’
  # means “any number of additional arguments is allowed”; when our
  # function is actually called with extra arguments,
  # R will store them for us so that we still can access and parse them
  # if we need to, but they will not be automatically assigned to
  # any named formal variables in the function
  # if we called our wrapper with ~. in the
  # formula, at this point the dot shortcut will be expanded into the
  # names of the variables provided in ‘data’; if we did not use the dot,
  # the formula will stay unchanged, that’s why we needed expand.formula()
  # to be “smart” about it:
  f <- expand.formula(f, data)
  # if observations have NA in any measurement (variable), we will need
  # to drop that measurement as a whole - meaning from the dataframe ‘data’ *AND* from
  # the dependent variable in order to keep all the data in sync.
  # Let us see where the outcome data are hiding (it might be a separate external
  # variable rather then a column in the provided data frame!); in order to do that, we
  # first need to extract the name of outcome variable from the formula;
  # using the same approach as the one we used in expand.formula() earlier, we get:
  f.str <- deparse(f)
  # if the formula is too long (which it might become after we explicitly
  # inserted the names of all the columns), deparse() may introduce
  # “line breaks”, i.e. instead of a single string it will return a
  # vector of strings; if this happened, here we collapse them back:
  f.str <- paste(f.str, collapse = "")
  dependent.name <- sub("\\s*~.*", "", f.str)
  # if the dependent variable is found among the columns of ‘data’, we will
  # process it later. However, if the dependent variable is not in ‘data’
  # but is supposed to be found in the environment, we got some magic to do:
  if (!dependent.name %in% names(data)) {
    # Imagine that the dependent.name variable
    # holds the string value "outcome" (i.e. the formula was “outcome ~ ...”).
    # Thus when this formula is used for fitting, the variable with the name
    # ‘outcome’ will be looked up in the environment the formula points to.
    # The get() command takes the name of the variable
    # to extract the value for, and the environment to extract it from, so we copy into
    # the local variable dependent.data the value of the variable ‘outcome’!
    # The beauty of it is that the formula can contain any name of the
    # dependent variable of course; we extract that *name* at run-time,
    # and pull the value of the variable with *that* specific name from
    # Note that this won’t work if the lhs of the formula contains some
    # rather than just the variable name, e.g. sqrt(Y) or as.factor(Y);
    # this can be fixed, but we don’t need it right now.
    # Following line of code extracts the value of the variable with the *name* stored
    # in another variable (called dependent.name), from a specified environment:
    dependent.data <- get(dependent.name, envir = environment(f))
    # we now have a copy of dependent variable data vector in our hands,
    # so we can filter and un-factor it as we please; but the formula still
    # refers to the original variable (“outcome” in our example),
    # how can we make the formula refer to *our* local data instead?
    # Overwriting the value of the original outcome variable is possible,
    # but that means corrupting data in the global environment – a very bad
    # idea. Instead, we will bind the outcome data vector to our
    # local copy of the ‘data’ (remember, here we are dealing with the case
    # when the outcome variable is NOT in the data frame, so it’s ok).
    # The model-fitting function is going to look in the dataframe first, before looking
    # in the enclosing environment, so it will work just like we want it:
    data <- cbind(dependent.data, data)
    # set the column name to the name of outcome used in the formula:
    names(data)[1] <- dependent.name
    # now the model will find dependent data in the dataframe!
  }
  # now the dataframe ‘data’ definitely keeps the outcome variable too
  # either because it did so from the start, or because we found those
  # outcome data and manually added them to the dataframe. All that is
  # is to remove rows with NA from ‘data’ and un-factor the outcome if
  # not like factors either):
  has.na <- apply(data, 1, function(x) {
    any(is.na(x))
  })
  # use drop=F to ensure that even if the data are actually a matrix and the result of the
  # following operation is just one row, that row is still kept as (one-row) matrix and
  # not auto-converted to a vector (that’s what R would normally do by default):
  data <- data[!has.na, , drop = F]
  if (is.factor(data[, dependent.name])) {
    data[, dependent.name] <-
      as.numeric(as.vector(data[, dependent.name]))
  }
  # now call the original neuralnet function with the fixed formula and data.
  # The “...” in the function call means: take all
  # the extra parameters we grabbed with ‘...’ in the function definition above,
  # and stick them all here, i.e. we are just passing them all through :
  return(neuralnet(f, data, ...))
}
```


# Problem 1 (10 points): generate 3D data with nested cubes

Simulate data with n=1000 observations and p=3 covariates -- all random variables from uniform distribution contained on $[-1,1]$ interval: $X_i\sim \mathcal{U}(-1,1),i=\{1,2,3\}$.  Resulting observations should be uniformly filling in a 3D cube centered at $(0,0,0)$ with the edge of length $2$. Create two category class variable assigning all observations within a smaller cube -- also centered at $(0,0,0)$ with the edge length of $2^\frac{2}{3}$ -- to one class category and observations outside of this cube -- to the second class category.

Please note that this dataset is entirely different from the one used in the preface -- you will need to write code simulating it on your own.  Somewhat related 2D dataset was used as a motivational example at week 12 (SVM) lecture before introducing kernels and SVMs. However, the example in the lecture was in 2D (three-dimensional problem here), had spheric boundary  (here we work with nested cubes) and used normal (here uniform) distribution.  Since you will be reusing this code in the following two problems it is probably best to turn this procedure into a function with appropriate parameters.

Ten points available for this problem are composed of accomplishing the following tasks:

## Questions

1. Correct implementation of the function generating data as prescribed above (2 points)
  + See [Data generation function](#data-generation-function) section and its subsections
2. Check and demonstrate numerically that the resulting class assignment splits these observations, subject to sampling variability, evenly between these two groups (2 points)
  + Output in the [function testing](#function-testing) and [data generation](#generate-actual-data) has both tables and proportions for a few different variations.
  + All output I've seen is $\approx 50\pm5\%$
3. Explain why this is achieved by using $2^\frac{2}{3}$ as the length of the inner cube edge that defines true decision boundary (2 points)
  + For $p$ dimensions of length $L$, an even split with 2 classes requires an edge length of $\frac{L}{2} \times 2^\frac{p - 1}{p}$
  + In our example with $p = 3; L = 2$:
    + Proportion in-range along any one dimension is $\frac{2^\frac{2}{3}}{2} = 0.7937$
    + Along 2 dimensions is $0.7937^2 = 0.6300$
    + Along 3 dimensions is $0.7937^3 = 0.5000$
    + $(\frac{2^\frac{2}{3}}{2})^3 = \frac{2^2}{2^3} = \frac{4}{8} = 0.5$
  + In general: $(\frac{\frac{L}{2} \times 2^\frac{p - 1}{p}}{L})^p = (\frac{2^\frac{p - 1}{p}}{2})^p = \frac{2^{p-1}}{2^p} = \frac{2^{0}}{2^1} = \frac{1}{2}$
4. Plot values of the resulting covariates projected at each pair of the axes indicating classes to which observations belong with symbol color and/or shape (you can use function `pairs`, for example) (2 points)
 + See [data generation](#generate-actual-data) section
5. Reflect on the geometry of this problem by answering the following question: what is the smallest number of planes in 3D space that would completely enclose points from the "inner" class?  Is this number equal to the number of cube faces or is it something smaller? Larger?  Please note that "enclose" above does *not* mean "perfectly discriminate between the points assigned to two classes" (2 points) 
  + In 3D space, we would need 1 plane per face of the cube
  + If projection into 4 or more dimensions is allowed, we may be able to get away with a single plane
    + See 2D to 3D example in [function testing](#function-testing) section

## Data generation function

+ Inputs
  + `n`: number of observations (default `1000`)
  + `p`: numer of covariates/data variables (default `3`)
  + `noise.var`: number of noise/uncorrelated variables (default `0`)
  + `limits`: endpoints for the uniform distribution (defaul `c(-1,1)`)
  + `print.info`: boolean for printing additional info (default `FALSE`)
+ Printed data for `print.info`
  + table of `class` values and proportion class `1`
  + `head()` of data frame
  + 3D plot of data colored by class
+ Returned data frame
  + `n` rows
  + `p + n + 1` columns
    + `p#` for correlated
    + `n#` for noise
    + `class` for assigned classification (`0`/`1`)

### Function definition

```{r Generate data}
generate.data <- function(n = 1000, p = 3, noise.var = 0, limits = c(-1,1), print.info = FALSE){
  # Check user inputs:
  # Meaningful values
  stopifnot(p > 0, n > 0, noise.var >= 0, length(limits) == 2)
  # Whole numbers
  stopifnot(sapply(c(p, n, noise.var, limits[1], limits[2]),
                   function(x){all.equal(x, as.integer(x))}))
  # min < max
  stopifnot(limits[1] < limits[2])
  
  # Data frame for results
  d <- data.frame(matrix(nrow = n, ncol = p + noise.var + 1))
  
  # Generate data
  for (i in 1:p){
    d[, i] <- runif(n, limits[1], limits[2])
    colnames(d)[i] <- paste0("p", i)
  }
  # Generate noise
  if (noise.var > 0) {
    for (i in 1:noise.var){
      d[, i + p] <- runif(n, limits[1], limits[2])
      colnames(d)[i + p] <- paste0("n", i)
    }
  }
  
  # Generate classification (~50/50 split)
  colnames(d)[p + noise.var + 1] <- "class"
  half.edge <- (limits[2] - limits[1]) / 2
  midpoint <- limits[1] + half.edge
  # Calculate edge length needed based on number of variables
  divider <- (half.edge * (2 ^ ((p - 1) / p)) / 2)
  temp <- rep(TRUE, times = n)
  for (i in 1:p){
    # Shift values so that square is centered if limits not symmetric
    temp <- temp & (abs(d[i] - midpoint) < divider)
  }
  d$class <- as.numeric(temp)
  
  # Print info if indicated
  if (print.info) {
    print(table(d$class))
    print(paste("Proportion class 1:", sum(d$class) / n))
    print(head(d))
    if (p == 2){
      plot(d$p1, d$p2, col = (2 * d$class) + 2, pch = d$class)
    }
    else if (p >= 3){
      scatterplot3d(d$p1, d$p2, d$p3, color = (2 * d$class) + 2, pch = d$class)
    }
  }
  
  # Return data.frame
  return(d)
}
```

### Function testing

Check error handling, performance with modified options

```{r test out function}
# Testing:
# Catches errors
# generate.data(n = -1)
# generate.data(p = 3.14)
# generate.data(limits = c(1, -1))

# Low-dimenstion data
t1 <- generate.data(p = 2, limits = c(-3, 5), print.info = TRUE)

# High-dimensional (only plots first 3)
t2 <- generate.data(n = 2500, p = 4, print.info = TRUE)

# Noisy data
t3 <- generate.data(n = 500, p = 2, noise.var = 1, print.info = TRUE)
scatterplot3d(t3$p1, t3$n1, t3$p2, color = (2 * t3$class) + 2, angle = 65)

# Example of better (if not perfect) splitting by projecting into extra dimensions
pl <- scatterplot3d(t3$p1, t3$p2, cos(t3$p1) * cos(t3$p2), color = (2 * t3$class) + 2)
pl$plane3d((2^(2/3))/2, 0, 0, lty = 3)
```

### Generate actual data

```{r Data to use}
d <- generate.data(print.info = TRUE)
ggpairs(data = d[,-4], aes(color = factor(d$class), alpha = 0.6), progress = FALSE)
```


# Problem 2 (20 points): neural network classifier

For the dataset simulated above fit neural networks with 1 through 6 nodes in a single hidden layer (use `neuralnet` implementation).  For each of them calculate training error (see an example in Preface where it was calculated using `err.fct` field in the result returned by `neuralnet`).  Simulate another independent dataset (with n=10,000 observations to make resulting test error estimates less variable) using the same procedure as above (3D, two classes as nested cubes) and use it to calculate the test error at each number of hidden nodes.  Plot training and test errors as function of the number of nodes in the hidden layer.  What does resulting plot tells you about the interplay between model error, model complexity and problem geometry?  What is the geometrical interpretation of this error behavior?

## Code

### Helper function for error/accuracy

```{r Error and accuracy}
calc.error <- function(nn.model, new.data, tbl.print = FALSE) {
  pred <- predict(nn.model, newdata = new.data)
  tbl <- table(actual = new.data$class,
               predicted = as.numeric(pred > 0.5))
  acc <- sum(diag(tbl)) / sum(tbl)
  # Calculate training error
  err <- sum(nn.model$err.fct(pred, new.data$class), na.rm = TRUE)
  if (tbl.print){
    print(paste("Accuracy:", acc))
    print(tbl)
  }
  # Return error and accuarcy in data fram
  return(data.frame(error = err / nrow(new.data), accuracy = acc))
}
```

### Testing number of nodes

```{r NN Classifier, cache=TRUE}
# For consistency. Added after parameters were tuned
set.seed(123)

nn.results <- data.frame(
  nodes = numeric(),
  error = numeric(),
  accuracy = numeric(),
  type = character()
)
d.test <- generate.data(n = 10000)

for (n in 1:10) {
  # Generate model on training data
  nn.train <-
    neuralnet(
      class == 1 ~ p1 + p2 + p3,
      data = d,
      hidden = n,
      err.fct = "ce",
      linear.output = FALSE,
      threshold = 0.5
    )
  # Calculate training error and accuracy, then store everything
  train.err.acc <- calc.error(nn.train, d)
  nn.results <- rbind(nn.results,
                      cbind(train.err.acc, nodes = n, type = "train"))
  
  # Calculate test error, print table
  print(paste("Number of nodes:", n))
  test.err.acc <- calc.error(nn.train, d.test, tbl.print = TRUE)
  nn.results <- rbind(nn.results,
                      cbind(test.err.acc, nodes = n, type = "test"))
}
```



``` {r plots for effect of node numbers}
ggplot(data = nn.results, aes(x = nodes, y = error, color = type)) + 
  geom_point(size = 3) + 
  geom_line() + 
  scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) +
  labs(title = "Normalized Error vs Number of Nodes", 
       x= "Number of Nodes", 
       y = "Normalized Error",
       type = "Train/Test")

ggplot(data = nn.results, aes(x = nodes, y = accuracy, color = type)) + 
  geom_point(size = 3) + 
  geom_line() + 
  scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) +
  labs(title = "Accuracy vs Number of Nodes", 
       x= "Number of Nodes", 
       y = "Accuracy",
       type = "Train/Test")
```

## Discussion

**Notes**:
An increase of the `threshold` value from the default `0.01` was needed to get reliable convergence. Using `0.05` worked, but `0.1` is more reliable using `err.fct = "sse"`. If `err.fct = "ce"` is used, raising `threshold` to `0.25` usually worked, but `0.5` always worked and usually gave the best results. The other option was setting `stepmax = 1e7`, which could result in rather long runs.

**What does resulting plot tells you about the interplay between model error, model complexity and problem geometry?**

Model error decreases with increasing complexity to a point for both training and test sets. Going beyond 6 hidden nodes doesn't seem to help training or test error, and even increases the test error, likely due to overfitting.

**What is the geometrical interpretation of this error behavior?**

Since a cube has 6 faces, 6 planes are needed to differentiate the interior points from exterior points.

note to me: What happens if you add a transform to another dimension? Try if there's time.

# Problem 3 (30 points): evaluate impacts of sample size and noise

Setup a simulation repeating procedure described above for n=100, 200 and 500 observations in the *training* set as well as adding none, 1, 2 and 5 null variables to the training and test data (and to the covariates in formula provided to `neuralnet`).  Draw values for null variables from uniform distribution on $[-1,1]$ as well and do not use them in the assignment of the observations to the class category (e.g. `x<-matrix(runif(600,-1,1),ncol=6); cl<-as.numeric(factor(rowSums(abs(x[,1:3])<2^(-1/3))==3))` creates dataset with three informative and three null variables). Repeat calculation of training and test errors at least several times for each combination of sample size, number of null variables and size of the hidden layer simulating new training and test dataset every time to assess variability in those estimates.  Present resulting error rates so that the effects of sample size and fraction of null variables can be discerned and discuss their impact of the resulting model fits.  

## Code

### Neural nets for varying numbers of observations, nodes, and noise variables

```{r samples size and noise, cache=TRUE}
# Data frame for results
nn.samp.noise <- data.frame(
  n.train = numeric(),
  nodes = numeric(),
  noise.var = numeric(),
  error = numeric(),
  accuracy = numeric(),
  type = character()
)

for (x in 1:25) {
  # Print lots of stuff?
  verbose <- FALSE
  
  # Fit neural nets and calculate error for:
  # 1 to 10 nodes to be consistent with change above
  for (n.nodes in 1:10) {
    # 100, 200, and 500 observations
    for (n in c(100, 200, 500)) {
      # 0, 1, 2, and 5 uncorrelated variables
      for (noise.var in c(0, 1, 2, 5)) {
        # Generate training and test data with appropriate n and noise
        train.data <- generate.data(n = n, noise.var = noise.var)
        test.data <- generate.data(n = 10000, noise.var = noise.var)
        
        # Fit neural net model using unwrapper helper function
        nn.temp <- neuralnet.fx(
          class ~ .,
          data = train.data,
          hidden = n.nodes,
          err.fct = "ce",
          linear.output = FALSE,
          threshold = 0.5
        )
        
        # Calculate training error and accuracy, then store everything
        train.err <- calc.error(nn.temp, train.data)
        new.train <-
          data.frame(
            error = train.err$err,
            accuracy = train.err$acc,
            n.train = n,
            nodes = n.nodes,
            noise.var = noise.var,
            type = "train"
          )
        nn.samp.noise <- rbind(nn.samp.noise, new.train)
        
        # Print table
        if (verbose){
          print(
            paste0(
              "Nodes: ",
              n.nodes,
              ", Training observations: ",
              n,
              ", Noise variables:",
              noise.var
            ))
        }
        
        # Calculate test error and accuracy, print table, then store everything
        test.err <- calc.error(nn.temp, test.data, tbl.print = verbose)
        new.test <-
          data.frame(
            error = test.err$err,
            accuracy = test.err$acc,
            n.train = n,
            nodes = n.nodes,
            noise.var = noise.var,
            type = "test"
          )
        nn.samp.noise <- rbind(nn.samp.noise, new.test)
      }
    }
  }
}
```

### Plots

Resource: https://stackoverflow.com/questions/16026215/generate-ggplot2-boxplot-with-different-colours-for-multiple-groups

```{r Plots for samples size and noise, fig.height=6, fig.width=8}
# Error
for (n in c(100, 200, 500)) {
  temp <- subset(nn.samp.noise, subset = n.train == n)
  g <- ggplot(data = temp, aes(x = nodes, y = error, fill = type, group = interaction(type, nodes))) + 
  facet_wrap(~noise.var, scales = "free") + 
  geom_boxplot() +
  scale_x_continuous(breaks = seq(2, 8, by = 2)) +
  labs(title = "Normalized Error vs Number of Nodes Tiled by Number of Noise Variables", 
       subtitle = paste("Number of training observations:", n),
       x= "Number of Nodes", 
       y = "Normalized Error",
       shape = "Training Observations", 
       color = "Train/Test")
  print(g)
}

```

## Discussion

TODO

# Extra 10 points problem: model WiFi localization data

Use `neuralnet` to model the outcome in WiFi localization dataset that we used in previous weeks *using outcome in its original, four-levels format.* Your `neuralnet` models will need to have four output nodes to represent outcome with four levels. Obtain training and test error for this model for several sizes of the hidden layer.  Compare resulting test error in predicting Location 3 to that observed for random forest, SVM and KNN approaches earlier in the course.

## Code

### Data loading and prep

```{r Wifi}
# Load data
wifi <- read.table("wifi_localization.txt", sep = "\t")
colnames(wifi) <- c(paste0("WiFi", 1:7), "Loc")
# Make separate columns for each location
for (i in 1:4){
  cols <- ncol(wifi)
  wifi[cols + 1] <- wifi$Loc == i
  colnames(wifi)[cols + 1] <- paste0("L", i)
}
# Store, then remove "Loc"
loc <- wifi$Loc
wifi <- subset(wifi, select = -Loc)
# Formula for use with neuralnet()
form <- as.formula(paste("L1 + L2 + L3 + L4 ~", paste(names(wifi)[1:7], collapse = " + ")))
```

### Neural net with one hidden layer

```{r Neural Net with one layer, cache=TRUE}
results.1 <- data.frame(nodes = numeric(),
                        error = numeric())

for (n.nodes in c(1:8, 12, 16, 24, 32)) {
  nn.1 <- neuralnet(
    formula = form,
    data = wifi,
    hidden = n.nodes,
    err.fct = "sse",
    linear.output = TRUE,
    threshold = 0.5
  )
  results.1 <-
    rbind(results.1,
          data.frame(nodes = n.nodes,
                     error = sum(
                       nn.1$err.fct(nn.1$net.result[[1]], nn.1$response)
                     )))
}
```

### Neural net with 2 hidden layers

```{r Neural Net with 2 layers, cache=TRUE}
results.2 <- data.frame(layer1 = numeric(),
                        layer2 = numeric(),
                        error = numeric())
for (n1 in 2 ^ (1:4)) {
  for (n2 in 2 ^ (1:4)) {
    nn.1 <- neuralnet(
      formula = form,
      data = wifi,
      hidden = c(n1, n2),
      err.fct = "sse",
      linear.output = TRUE,
      threshold = 0.5,
      stepmax = 1e6
    )
    results.2 <-
      rbind(results.2,
            data.frame(layer1 = n1,
                       layer2 = n2,
                       error = sum(
                         nn.1$err.fct(nn.1$net.result[[1]], nn.1$response)
                       )))
  }
}
```

### Plots

```{r Wifi Error plots}
ggplot(data = results.1, aes(x = nodes, y = error)) + 
  geom_point(size = 3) + 
  geom_line() +
  scale_x_continuous(breaks = seq(2, 32, by = 2)) +
  scale_y_log10() +
  labs(title = "Error vs Number of Nodes in 1 layer Neural Net",
       x = "Number of Nodes",
       y = "Error")

ggplot(data = results.2, aes(x = layer1, y = error, color = as.factor(layer2))) + 
  geom_point(size = 3) + 
  geom_line() +
  scale_x_continuous(trans = "log2") +
  labs(title = "Error vs Number of Nodes in 2 layer Neural Net",
       x = "Layer 1 Nodes",
       y = "Error", 
       color = "Layer 2 Nodes")
```


## Discussion

For the single-layer model, all nodes from 1 to 7 were chosen to see what happened when the number of nodes was at or below the number of variables. Additional nodes (8, 12, 16, 24, 32) were chosen to see if any extra would improve performance.

As may be expected, the neural network has a large decrease in error at 7 nodes, equal to the number of input variables. Having 8 nodes seems to pick up extra error, while 12, 16, 24, and 32 ...



# Session info {-}

For reproducibility purposes it is always a good idea to capture the state of the environment that was used to generate the results:

```{r}
sessionInfo()
```

The time it took to knit this file from beginning to end is about (seconds):

```{r}
# I cache chunks as I'm done with them!
proc.time() - ptStart
```
