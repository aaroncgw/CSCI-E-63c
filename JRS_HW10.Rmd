---
title: "CSCI E-63C Week 10 Problem Set"
author: 'Joshua Sacher'
date: '`r Sys.Date()`'

output:
  html_document:
    df_print: kable
    number_sections: true
    toc: true
    toc_float:
      collapsed: false
---

# Preface

For this week problem set we will use WiFi localization data (the one we worked with on week 2) to fit logistic regression model and evaluate performance of LDA, QDA and KNN classifiers.  As we have seen earlier this dataset should allow to locate phones fairly well by relying on the strength of WiFi signal, so we should expect to see fairly low error rates for our classifiers.  Let's see whether some of those classifiers perform better than others on this data.

**Important note:** *For the purposes of all problems in this week problem set, we will be predicting whether the phone is at location=3 or not, as opposed to working with multi-class predictor.  In other words, before you proceed with any of the problems in this assignment, please convert the four-levels outcome to the outcome with only two levels: location=3 (must be 500 of those) and not (must be 1500 of them).*

*If you are creating a new column containing this binary outcome, please make sure that the original outcome with four columns is NOT used inadvertently as one of the predictors.  If you are getting invariably 100% accuracy regardless of the choice of the method or split of the data into training and test, chances are your code is using original four-levels outcome as a predictor.*

```{r setup, include=FALSE}
library(class)
library(cowplot)
library(e1071)
library(ggplot2)
library(GGally)
library(MASS)
knitr::opts_chunk$set(echo = TRUE, fig.height = 15, fig.width = 20)
```

# Problem 1 (10 points): logistic regression

Fit logistic regression model of the binary categorical outcome (location=3 or not) using seven WiFi signals strengths as predictors in the model.  Produce summary of the model, describe which attributes appear to be significantly associated with the categorical outcome in this model.  Use this model to make predictions on the entire dataset and compare these predictions and corresponding true values of the class attribute using confusion matrix (i.e. contingency table).  Calculate error rate (would this be training or test error in this case?), sensitivity and specificity (assuming that we are predicting class "location=3").  Describe the results.

## Load and prep data

```{r Load and transform data}
# Read in space-separated data
wifi <- read.table("wifi_localization.txt", col.names = c(1:7, "room"))
# Convert to room 3 / not room 3
wifi$is.3 <- wifi$room == 3
wifi <- subset(wifi, select = -room)
# Sanity check -- should be 500 room 3 observations
sum(wifi$is.3)

```

## Summary of data

### Code and plots

```{r Graphical Analysis}
# Stolen from my HW 2
# Plot pairwise graphs
ggpairs(
  wifi,
  aes(col = wifi$is.3, alpha = 0.5),
  lower = list(combo = wrap("facethist", binwidth = 0.5)),
  progress = FALSE
)

# A closer look at that last line of histograms
# Function to generate histograms
hists <- function(X) {
  p <- ggplot(wifi, aes(x = wifi[[X]], color = is.3, fill = is.3))
  p <- p + geom_histogram(binwidth = 0.5, position = "identity", alpha = 0.4)
  p <- p + labs(title = paste("Distribution of", X), x = "WiFi Signal")
  return(p)
}
# Use the function above for each of the 7 measurements
plots <- lapply(names(subset(wifi, select = -is.3)), hists)
# Plot as a grid
plot_grid(plotlist = plots, ncol = 2)
```

### Discussion

+ $X1$ and $X4$ seem to be the most useful in a positive discriminatory sense
  + If in a range roughly centered around -50 in both, highly likely to be in room 3
  + The two are highly correlated -- $0.92$ -- so they may have redundant information
+ Room 3 generally has a narrower distribution compared to the rest of the variables except $X2$
  + Overlapping region may have somewhat higher probability of being room 3, but can't be sure without multiple observations
  + This may be useful in classification, as high and low values indicate "not room 3"

## Helper function for classification analysis

Modified from example code in lecture slides

```{r}
assess.prediction <- function(actual, predicted, print.vals = TRUE) {
  # Convert to logical values if needed
  actual = as.logical(actual)
  predicted = as.logical(predicted)
  
  TP = sum(actual & predicted)
  TN = sum(!actual & !predicted)
  FP = sum(!actual & predicted)
  FN = sum(actual & !predicted)
  P = TP+FN # total number of positives in the actual data
  N = FP+TN # total number of negatives
  
  if (print.vals) {
    # For prettiness
    cat("\nCLASSIFIER PERFORMANCE\n")
    # Accuracy (TP + TN) / (TP + TN + FP + FN)
    cat("Accuracy:        ", round(100 * (TP + TN) / (P + N), 1), "%\n", sep ="")
    # Sensitivity TP / (TP + FN)
    cat("Sensitivity:     ", round(100 * TP / P, 1), "%\n", sep ="")
    # Specificity TN / (TN + FP)
    cat("Specificity:     ", round(100 * TN / N, 1), "%\n", sep ="")
    # Precision (Positive Predictive Value) TP / (TP + FP)
    cat("Precision:       ", round(100 * TP / (TP + FP), 1), "%\n", sep ="")
    # False discovery rate 1 - PPV
    cat("False Discovery: ", round(100 * FP / (TP + FP), 1), "%\n", sep ="")
    # False positive rate 1 - TNR
    cat("False Positive:  ", round(100 * FP / N, 1), "%\n", sep ="")
    cat("\n")
  }
  
  # Return table for printing
  return(table(actual = actual, predicted = predicted))
}
```


## Logistic regression

### Code

```{r}
logi <- glm(is.3 ~ ., data = wifi, family = "binomial")
summary(logi)

logi.pred <- predict(logi, type = "response") > 0.5

# How does the prediction do? 
assess.prediction(actual = wifi$is.3, predicted = logi.pred)

```

### Discussion

+ Logistic regression appears to be good at telling when the location is _not_ room 3, with 92% specificity.
+ Not great at predicting if the location is room 3
  + Only 178 out of 500 correctly predicted
  + 322 false negatives
  + Low sensitivity of 36%
+ Reasonable accuracy of 78% is driven by the high true negative rate

# Problem 2 (10 points): LDA and QDA

Using LDA and QDA implementations available in the package `MASS`, fit LDA and QDA classifiers on the entire dataset and calculate confusion matrix, (training) error rate, sensitivity and specificity for each of them.  Compare them to those of logistic regression.  Describe the results.

## LDA

```{r}
wifi.lda <- lda(is.3 ~ ., data = wifi)
wifi.lda

# Check LDA performance
lda.pred <- predict(wifi.lda)
assess.prediction(actual = wifi$is.3, predicted = lda.pred$class)

# Looks similar to logistic regression. How well do they directly compare? 
# Note "actual" is not real truth in this case!
assess.prediction(actual = lda.pred$class, predicted = logi.pred)
```

## QDA

```{r}
wifi.qda <- qda(is.3 ~ ., data = wifi)
wifi.qda

# Check QDA performance
qda.pred <- predict(wifi.qda)
assess.prediction(actual = wifi$is.3, predicted = qda.pred$class)
```

## Discussion

+ LDA is nearly identical to logitic regression. All metrics are well within $\pm 1 \%$.
  + The two methods generate nearly the same classification, with a difference of only 18 out of 2000 predictions.
+ QDA performs _much_ better than logistic regression or LDA. This is likely due to the the fact that room 3 doesn't have any nice, linear boundaries.
  + No clear "above or below this point" in any of the $X$'s
  + The values for the most differentiable $X$'s have room 3 in the center and not-room-3 at the extremes. Will necessitate a non-linear boundary.

# Problem 3 (10 points): KNN

Using `knn` from library `class`, fit KNN classifiers for the entire dataset and calculate confusion matrix, (training) error rate, sensitivity/specificity for  $k=1$, $5$ and $25$ nearest neighbors models.  Compare them to the corresponding results from LDA, QDA and logistic regression. Describe results of this comparison and discuss whether it is surprising to see low *training* error for KNN classifier with $k=1$.

## Code

```{r KNN}

for (k in c(1, 5, 25)) {
  knn.results <- knn(train = wifi, test = wifi, cl = wifi$is.3, k = k)
  cat("\nRESULTS FOR K = ", k, ":", sep = "")
  print(assess.prediction(actual = wifi$is.3, predicted = knn.results))
}

```

## Discussion

+ Using $K = 1$ as both training and test will necessarilly result in perfect accuracy, specificity, etc. A point's closest neighbor is itself!
+ $K = 5$ and $K = 25$ also perform well, representing an "improvement" over QDA -- the best so far.
+ I don't think these comparisons are very useful yet, as everything is classifying existing data. Performance on new data (or at least cross-validation, below) will be a better measure of the "best" classification method.

# Problem 4 (30 points): compare test errors of logistic regression, LDA, QDA and KNN

Using resampling approach of your choice (e.g. cross-validation, bootstrap, etc.) obtain test error as well as sensitivity and specificity for each of these methods (logistic regression, LDA, QDA, KNN with $k=1,7,55,351$).  Present results in the form of boxplots, compare test error/sensitivity/specificity across these methods and discuss their relative performance.

## Code for cross validation

```{r Horrific code for cross-validation, cache=TRUE}
# Helper function for accuracy, sensitivity, and specificity
stats <- function(actual, predicted){
  temp <- assess.prediction(actual = actual,
                            predicted = predicted,
                            print.vals = FALSE)
  acc <- (temp[1, 1] + temp[2, 2]) / sum(temp)
  sen <- temp[2, 2] / (temp[2, 1] + temp[2, 2])
  spe <- temp[1, 1] / (temp[1, 1] + temp[1, 2])
  return(c(acc = acc, sen = sen, spe = spe))
}

metrics = c("accuracy", "sensitivity", "specificity")

# Collect ALL THE INFO here
results <- data.frame(
  method = character(),
  metric = character(),
  split = character(),
  value = numeric()
)

# I hate myself for writing this code. Sorry.

nrep <- 1000
# For each model
for (i in 1:6)
  # Cross validate nrep times
  for (j in 1:nrep) {
    # Approximately 80/20 split for train/test
    train.data <- sample(c(TRUE, FALSE), size = nrow(wifi), 
                         replace = TRUE, prob = c(0.8, 0.2))
    train <- wifi[train.data, ]
    test <- wifi[!train.data, ]
    
    # logistic regression
    if (i == 1) {
      # Generate model and get training/test statistics
      model <- glm(is.3 ~ ., data = train, family = "binomial")
      train.stats <- stats(actual = train$is.3, 
                           predicted = predict(model, newdata = train) > 0.5)
      test.stats <- stats(actual = test$is.3, 
                          predicted = predict(model, newdata = test) > 0.5)
      # Add those stats to the results data frame
      for (val in 1:length(train.stats)) {
        results <- rbind(results, data.frame(method = "logistic", metric = metrics[val],
                                             split = "train", value = train.stats[val]))
      }
      for (val in 1:length(test.stats)) {
        results <- rbind(results, data.frame(method = "logistic", metric = metrics[val],
                                             split = "test", value = test.stats[val]))
      }
    }
    # LDA
    else if (i == 2) {
      model <- lda(is.3 ~ ., data = train)
      train.stats <- stats(actual = train$is.3, 
                           predicted = predict(model, newdata = train)$class)
      test.stats <- stats(actual = test$is.3, 
                          predicted = predict(model, newdata = test)$class)
      for (val in 1:length(train.stats)) {
        results <- rbind(results, data.frame(method = "lda", metric = metrics[val],
                                             split = "train", value = train.stats[val]))
      }
      for (val in 1:length(test.stats)) {
        results <- rbind(results, data.frame(method = "lda", metric = metrics[val],
                                             split = "test", value = test.stats[val]))
      }
    }
    # QDA
    else if (i == 3) {
      model <- qda(is.3 ~ ., data = train)
      train.stats <- stats(actual = train$is.3, 
                           predicted = predict(model, newdata = train)$class)
      test.stats <- stats(actual = test$is.3, 
                          predicted = predict(model, newdata = test)$class)
      for (val in 1:length(train.stats)) {
        results <- rbind(results, data.frame(method = "qda", metric = metrics[val],
                                             split = "train", value = train.stats[val]))
      }
      for (val in 1:length(test.stats)) {
        results <- rbind(results, data.frame(method = "qda", metric = metrics[val],
                                             split = "test", value = test.stats[val]))
      }
    }
    # KNN
    else if (i == 4) {
      for (k in c(1, 7, 55, 351)){
        train.stats <- stats(actual = train$is.3, 
                           predicted = knn(train = train, test = train, cl = train$is.3, k = k))
        test.stats <- stats(actual = test$is.3, 
                          predicted = knn(train = train, test = test, cl = train$is.3, k = k))
        for (val in 1:length(train.stats)) {
          results <- rbind(results, data.frame(method = paste("KNN K =", k), metric = metrics[val],
                                               split = "train", value = train.stats[val]))
        }
        for (val in 1:length(test.stats)) {
          results <- rbind(results, data.frame(method = paste("KNN K =", k), metric = metrics[val],
                                               split = "test", value = test.stats[val]))
        }
      }
    }
    # naive Bayes
    else if (i == 5) {
      model <- naiveBayes(is.3 ~ ., data = train)
      train.stats <- stats(actual = train$is.3, 
                           predicted = predict(model, newdata = train))
      test.stats <- stats(actual = test$is.3, 
                          predicted = predict(model, newdata = test))
      for (val in 1:length(train.stats)) {
        results <- rbind(results, data.frame(method = "naiveBayes", metric = metrics[val],
                                             split = "train", value = train.stats[val]))
      }
      for (val in 1:length(test.stats)) {
        results <- rbind(results, data.frame(method = "naiveBayes", metric = metrics[val],
                                             split = "test", value = test.stats[val]))
      }
    }
    # logistic with interaction terms
    else if (i == 6) {
      model <- suppressWarnings(glm(is.3 ~ . * ., data = train, family = "binomial"))
      train.stats <- stats(actual = train$is.3, 
                           predicted = predict(model, newdata = train, type = "response") > 0.5)
      test.stats <- stats(actual = test$is.3, 
                          predicted = predict(model, newdata = test, type = "response") > 0.5)
      
      for (val in 1:length(train.stats)) {
        results <- rbind(results, data.frame(method = "logistic.interaction", metric = metrics[val],
                                             split = "train", value = train.stats[val]))
      }
      for (val in 1:length(test.stats)) {
        results <- rbind(results, data.frame(method = "logistic.interaction", metric = metrics[val],
                                             split = "test", value = test.stats[val]))
      }
    }
  }


```

## Plots

```{r Cross validation plots}
# https://stackoverflow.com/questions/27433798/how-can-i-change-the-y-axis-figures-into-percentages-in-a-barplot

ggplot(results, aes(x = factor(method), y = value, color = metric)) + 
  geom_boxplot() + 
  facet_wrap(~split) +
  theme_bw() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Classifier Method", title = paste("80/20 Cross Validation,", nrep, "Replicates")) +
  theme(text = element_text(size = 20), axis.text.x = element_text(angle = 90))

# Zoom in a bit
ggplot(results, aes(x = factor(method), y = value, color = metric)) + 
  geom_boxplot() + 
  facet_wrap(~split) +
  theme_bw() +
  scale_y_continuous(labels = scales::percent, limits = c(0.75, 1)) +
  labs(x = "Classifier Method", title = paste("80/20 Cross Validation,", nrep, "Replicates: >75%")) +
  theme(text = element_text(size = 20), axis.text.x = element_text(angle = 90))

# Zoom in more!
ggplot(results, aes(x = factor(method), y = value, color = metric)) + 
  geom_boxplot() + 
  facet_wrap(~split) +
  theme_bw() +
  labs(x = "Classifier Method", title = paste("80/20 Cross Validation,", nrep, "Replicates: >90%")) +
  scale_y_continuous(labels = scales::percent, limits = c(0.9, 1)) +
  theme(text = element_text(size = 20), axis.text.x = element_text(angle = 90))

```

## Discussion

+ As expected, the range of all metrics in the test set is broader than in the training set
+ Median values for test data are surprisingly similar to training medians
  + Exception is KNN with $K = 1$, as training data will necessarily be 100% accurate, sensitive, and specific.
+ Logistic regression and Linear Discriminant Analysis both have similar accuracy (~77%) and very low sensitivity. Specificity is not bad, but is lower than other methods
+ Quadratic Discriminant Analysis performs much better than it's linear counterpart
+ KNN performs very well
  + Lower values of $K$ perform better
  + Test metrics for $K = 1$ are better than I expected, but it makes sense as it's matching a point to its closest neighbor
+ Naive Bayes outperforms logistic regression and LDA. It also outperforms QDA in accuracy and sensitivity, but is slightly lower in specificity.
+ Logistic regression with all interaction terms significantly improves on logistic regression without
+ Overall, KNN with a relatively low value of $K$ (between 1 and 10ish) is likely the best
  + KNN performs best with relatively low-dimensional data. Having 7 $X$ values makes it a particularly suitable method for this data set

# Extra 5 points problem: naive Bayes classifier

Fit naive Bayes classifier (see lecture slides for examples of using `naiveBayes` function from package `e1071`) to the WiFi localization dataset with binary (location=3 or not) outcome and assess its performance on test data by resampling along with logistic regression, LDA, QDA and KNN in the Problem 4 above.

+ Naive Bayes above, but, for comparison, here are the results from the full data set

```{r}
wifi.nb <- naiveBayes(is.3 ~ ., data = train)
assess.prediction(actual = wifi$is.3,
                  predicted = predict(wifi.nb, newdata = wifi))

```


# Extra 10 points problem: interaction terms in logistic regression

Add pairwise interaction terms to the logistic regression model fit in the Problem 1 above and evaluate impact of their addition on training **and** test error.  You can add all pairwise interaction terms or a subset of them, in which case the rationale behind selecting such a subset has to be described in your solution.

+ Logistic with interaction terms above, but, for comparison, here are the results from the full data set

```{r}
wifi.inter <- glm(is.3 ~ . * ., data = wifi, family = "binomial")
assess.prediction(actual = wifi$is.3,
                  predicted = predict(wifi.inter, newdata = wifi,
                  type = "response") > 0.5)
```

