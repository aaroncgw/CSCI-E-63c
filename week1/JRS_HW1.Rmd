---
title: 'CSCI E-63C: Week 1 Problem Set'
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries
```{r load libraries, message=FALSE, warning=FALSE}
library(ggplot2)
library(cowplot)
```

# Problem 1 (30 points).

In class we have developed a simple simulation, in which we were looking at the mean of a sample as a random variable. Specifically, we were repeatedly drawing samples of size $N=20$ from the same underlying normal distribution. In order to observe how the sample mean fluctuates from one experiment to the next we have simply plotted a histogram of the obtained mean values. In this problem, we will characterize the distribution of those sample means with its standard deviation AND examine how the spread of that distribution (i.e. the distribution of the *sample means*) decreases with increasing sample size, in line with quite intuitive notion that if we draw a larger sample, then its mean is expected to be closer, at least on average, to the true mean of the underlying population the sample was drawn from. The skeleton of the R code is presented below in the RMD document. Notice that its evaluation is turned off by `eval=FALSE` code chunk parameter because it is incomplete and will fail otherwise -- once you modified the code so that it works, turn it to `eval=TRUE` (which is the default), so that it gets executed when you "Knit HTML":

## Problem 1 code
```{r sem}
# different sample sizes we are going to try:
sample.sizes <- c(3, 10, 50, 100, 500, 1000)

# we will use the vector below to save the standard deviations of the 
# *distribution of the means* at each given sample size.
# Note that it's ok to initialize with an empty vector of length 0 - if we index 
# it out-of-bounds while assigning a value later, the vector will autoexpand 
# on assignment, see examples in the slides) 
mean.sds <- numeric(0) 
sigma <- 15

for ( N in sample.sizes ) { # try different sample sizes

 # INSERT YOUR CODE HERE: (you may want to check the slides). 
  
 # 1) At each given N (i.e. in each iteration of the outer loop) you have to draw large number 
 # (e.g. 1000) of samples of that size N, from the distribution of your choice (e.g. normal, 
 # uniform, exponential, ...), and calculate the mean of *each* of those samples. 
 # Save all those means into a vector 'm'.
  
 m <- replicate(1000, mean(rnorm(n = N, mean = 100, sd = sigma)))
 # 2) Now, with vector m in hand, we want to characterize how much the sample mean fluctuates
 # from one experiment (experiment=drawing a sample of N measurements) to the next. Instead of just
 # plotting a histogram as we did in class, this time we will calculate the standard 
 # deviation of the distribution represented by the vector m. Use function sd() to achieve that.
 sd(m)
 # 3) save the result (sd of the distributions of the means for current N) into the vector mean.sds 
 # defined above. You can use c() to concatenate, or you can use an indexing variable; 
 # in the latter case you will need to add it to the code and increment properly
 # mean.sds <- append(mean.sds, c(sd(m)), after = length(mean.sds))
 mean.sds <- c(mean.sds, sd(m))
}

# at this point, you should have the vector mean.sds filled. It should have length 6 and keep the 
# calculated values of the standard deviation of the mean (known as the standard error of the mean, SEM) 
# at different sample sizes: mean.sds[1] is the SEM at N=3, mean.sds[2] is the SEM at N=10, and so on.
length(mean.sds)
```

## Plot with vanilla `R`
```{r Problem 1 plot}
# let us now plot the SEM (i.e. the "typical" error we expect the sample mean to exhibit in any 
# given experiment) as a function of the sample size, N. 

plot(sample.sizes, mean.sds, main = "SEM vs sample size", 
     pch = 19, xlab = "Sample size", ylab = "SEM")
lines(x = sample.sizes, y = sigma/sqrt(sample.sizes), col = 'blue')
```

In the last lines of the code shown above we introduced `plot()` function: the first argument is the vector of $x$-coordinates, the second argument is the vector of corresponding $y$-coordinates, and the function adds each data point $(x_i, y_i)$ to the plot. In our case, $x$ coordinates are sample sizes $N$ and $y$ coordinates are SEMs we just calculated. By default, `plot()` draws only data points themselves (without connecting lines, which also can be done). 
The last command calls the function `lines()` which is in fact a wrapper for the same function `plot()`, but has different defaults that are more convenient to us here: first, it does not start a new plot (which is default behavior of `plot()`), but instead adds to the existing one; second, it draws lines connecting the the data points. The data points we specify for this function are calculated according to the theoretical prediction: it can be shown that when samples of size $N$ are repeatedly drawn from a distribution with standard deviation $\sigma$, the standard error of the mean (i.e. the standard deviation of the *distribution of the means* of such samples) is $SEM=\frac{\sigma}{\sqrt{N}}$. 

Thus if you play with this code (please do!) and decide to try drawing samples from a distribution with a different standard deviation, do not forget to use correct $\sigma$ in the last drawing command (in the code above we are using `1/sqrt(sample.sizes)`, i.e. we assume that samples are drawn from the distribution with $\sigma=1$, just like we did in class when we used standard normal distribution with mean $\mu=0$ and standard deviation $\sigma=1$). HINT: your simulated SEM values should fall nicely onto the theoretical curve. If they don't, you got something wrong!

**For the full credit on this problem**, you have to practice working with R's documentation. Please see the docs (execute `help(plot)` or simply `?plot`) and find out what you need to add to the `plot()` command in the starter  code to set the axis labels. Your resulting plot **must** have X-axis labeled as "Sample size" and y axis labeled as "SEM". This last part will cost **5 points** towards the full 30 point credit for this problem.

If you prefer using `ggplot2` as your plotting facility in R (in which case you will know how to use `stat_function` to add theoretical curve to a scatterplot), please feel free to accomplish the above goals using it instead of base graphics plotting functions shown above.  

## Nicer (?) plot with `ggplot`
```{r problem 1 but with ggplot!}
ggplot(data = data.frame(sample.sizes, mean.sds), aes(sample.sizes, mean.sds)) + geom_point() + stat_function(fun = function(x) sigma/sqrt(x), col = "blue") + labs(title = "SEM vs sample size", x = "Sample size", y = "SEM")
```



# Problem 2 (30 points).

There is a beautiful fact in statistics called the Central Limit Theorem (CLT). It states that the distribution of a sum of $N$ independent, identically distributed (i.i.d.) random variables $X_i$ has normal distribution in the limit of large $N$, regardless of the distribution of the variables $X_i$ (under some very mild conditions, strictly speaking). 

Here is what it means in plain English: suppose we have a random variable distributed according to some arbitrary distribution $F(x)$ (i.e. we have a "measurement process" that returns values according to $F(x)$). Let's "measure" the variable, i.e. draw a value from that distribution, $x_1$. Then let us draw another value $x_2$ from the same distribution, independently, i.e. without any regard to the value(s) we have drawn previously. Continue until we have drawn $N$ values: $x_1, \ldots, x_N$. 

Let us now calculate the sum $s=\sum_1^Nx_i=x_1+\ldots+x_N$ and call *this* an "experiment". 
Clearly, $s$ is a realization of some random variable: if we repeat the experiment (i.e. draw $N$ random values from the distribution again) we will get a completely new realization $x_1, \ldots, x_N$ and the sum will thus take a new value too! Using our notations, we can also describe the situation outlined above as

$$S=X_1+X_2+\ldots+X_N, \;\; X_i \;\; \text{i.i.d.}$$

The fact stated by this equation, that random variable $S$ is the "sum of random variables" is just what we discussed above: the "process" $S$ is *defined* as measuring $N$ processes which are "independent and identically distributed" (i.e. draw from the same distribution, without any regard for the values that were already drawn in the past) and summing up the results.

We cannot predict, of course, what the sum is going to be until we perform the actual measurement of $X_1, \ldots, X_N$, so $S$ is indeed a random variable itself! Thus it has some probability distribution/probability density associated with it (some values of this sum are more likely than others), and what CLT tells us is that at large $N$ this distribution is bound to be normal, *regardless* of $F(x)$ we are drawing variables $X_i$ from.

Instead of proving CLT formally, let's simulate and observe it in action.

Here is initial code you will have to complete (remember about `eval=FALSE`!):

## Problem 2 code
```{r clt}
# Set up ggplot to use centered titles
theme_update(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5))

clt <- function(n){
  # how many times we are going to repeat the "experiment" (see the text above for what we now call an experiment):
  n.repeats <- 1000 
  s.values <- numeric() # we will use this vector to store the value of the sum, s, in each experiment
  
  for (i.exp in 1:n.repeats) { # repeat the experiment 'n.repeats' times
    # More details below. In each "experiment" we must draw the values x1, ..., xN of all the 
    # random variables we are going to sum up:
  
    ### replace with correct call: 
    x <- sum(runif(n))

  # the "measured" value of the random variable S in the current experiment is the sum of x1...xN;
  # calculate it and save into the vector s.values:
   
  ### replace with correct call: 
  s.values[i.exp] <- x
  }
  
  # we repeated the experiment 1000 times, so we have 1000 values sampled from the random variable ("process")
  # S and that should be plenty for looking at their distribution:
  
  ### replace with correct call: 
  
  # Test for normality (just for fun)
  shapiro.p <- round(shapiro.test(s.values)$p.value, digits = 4)
  
  # Generate plot (step by step):
  g <- ggplot(data = data.frame(s.values), aes(s.values))
  g <- g + labs(title = paste(n.repeats, "replicates of the sums of", n, "draws from a uniform PDF"), 
                x = paste("sum of values from", n, "draws"), 
                subtitle = paste("Shapiro-Wilk p-value:", shapiro.p)) 
  g <- g + geom_histogram(bins = 20, col = "darkblue", fill = "lightblue", alpha = 0.75) 
  # just for fun:
  g <- g + geom_freqpoly(bins = 40, col = "red")

  return(g)
}

# Define sequence of n values
N <- 2 ^ c(0:11)

# Generate a plot for each value in N
plots <- lapply(N, clt)

```

## Problem 2 plots
```{r Problem 2 plots, fig.width=12,fig.height=24}
# Display plots in a grid (yay cowplot!)
plot_grid(plotlist = plots, ncol = 2)

```

What you need to do is 

1. provide missing pieces indicated in the code skeleton above.  $\checkmark$
2. run the code for few different values of $N$, *in a loop* (i.e. not by copy-pasting the code manually), see below for the details. $\checkmark$ (`lapply()` instead of an actual `for` loop)

You should also remember that the sampling functions provided in R do just what we need. For instance, `rnorm(3)` will draw *three* values at once, independently, from the same normal distribution (with default $\mu=0$ and $\sigma=1$ in this particular example). But that's exactly what measuring three i.i.d normally distributed random variables is! So in order to sample our $N$ variables $X_1,\ldots,X_N$ in each experiment, we can just call the sampling function once, with $N$ as an argument (and whatever other arguments that specific DISTR function might require). 

**Please do not use** `rnorm()` for this problem though, it is too dull (the sum of *any* number of normally distributed variables, even just two, is again normal!). Use something very different from normal distribution. Uniform distribution or exponential (as implemented in R by`runif()` and `rexp()` functions) are good candidates (see help pages for the distribution function you choose in order to see what parameters it might require, if any).  It is also pretty entertaining to see the sum of discrete random variables (e.g. binomial) starting to resemble normal as $N$ increases!

Note that the starter code above uses $N=1$. In this case $S=X_1$ and obviously $S$ is thus the same "process" as $X_1$ itself. So the histogram at $N=1$ will in fact show you the distribution you have chosen for $X$. Loop over multiple values of $N$ to rerun the code a few times. See how the distribution of $S$ (the histogram we plot) changes for $N=2$, $N=5$, ... Can you see how the distribution quickly becomes normal even though the distribution we are drawing values from (the one you have seen at $N=1$) can be very different from normal?

Your solution for this problem **must include** histogram plots generated at few different $N$ of your choosing, at the very least for (1) $N=1$ (i.e. the distribution you choose to sample from), (2) $N$ large enough so that the distribution of $S$ in the histogram looks very "normal" , and (3) some intermediate $N$, such that distribution of $S$ already visibly departed from $N=1$ but is not quite normal just yet.  The plot titles **must indicate** which distribution and what sample size each of them represents.

## Problem 2 questions

Lastly, **for the full credit you should answer the following question (5 points)**: suppose you have an arbitrary distribution and take a sample of $N$ measurements from it. You calculate the mean of your sample. As we discussed, the sample mean is a random variable, of course.  

1) *How is the sample mean, as a random variable, distributed?*  
Given a large enough sample of means ($\ge 30$) (and helped by a larger $N$), sample means will be normally distributed.  

2) *What is __its__ (expected) mean (zero? infinity? constant? which one if so?)?*   
The expected mean will be the mean of the original distribution.  

3) *What about standard deviation of the sample mean? How does it behave as sample size $N$ increases?*  
As $N$ increases, the sample becomes more representative of the underlying population so the standard deviation of the sample mean will decrease.   

4) *Can anything be said about the shape of the distribution of sample means in the limit of large $N$?  HINT: look at the definition of the sample mean!*
As $N$ approaches the population size, the sample variance (and SD) will approach 0. This shows up as a narrowing of the distribution, theoreticall narrowing to 0 width when all the population has been measured.   

