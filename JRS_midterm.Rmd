---
title: "CSCI E-63C: Week 7 -- Midterm Exam"
author: 'Joshua Sacher'
date: '`r Sys.Date()`'

output:
  html_document:
    df_print: kable
    number_sections: true
    toc: true
    toc_float:
      collapsed: false
---

# Introduction

*The goal of the midterm exam is to apply some of the methods covered in our course by now to a new dataset.  We will work with the data characterizing real estate valuation in New Taipei City, Taiwan that is available at [UCI ML repository](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set) as well as at this course website on canvas. The overall goal will be to use data modeling approaches to understand which attributes available in the dataset influence real estate valuation the most.  The outcome attribute (Y -- house price of unit area) is inherently continuous, therefore representing a regression problem.*

*For more details please see dataset description available at UCI ML or corresponding [HTML file](https://canvas.harvard.edu/files/8396679/download?download_frd=1) in this course website on canvas.  For simplicity, clarity and to decrease your dependency on the network reliability and UCI ML or canvas website availability during the week that you will be working on this project you are advised to download data made available in this course canvas website to your local folder and work with this local copy. The dataset at UCI ML repository as well as its copy on our course canvas website is made available as an Excel file [Real estate valuation data set.xlsx](https://canvas.harvard.edu/files/8396680/download?download_frd=1) -- you can either use `read_excel` method from R package `readxl` to read this Excel file directly or convert it to comma or tab-delimited format in Excel so that you can use `read.table` on the resulting file with suitable parameters (and, of course, remember to double check that in the end what you have read into your R environment is what the original Excel file contains).*

*Finally, as you will notice, the instructions here are terser than in the previous weekly problem sets. We expect that you use what you've learned in the class to complete the analysis and draw appropriate conclusions based on the data.  The approaches that you are expected to apply here have been exercised in the preceeding weeks -- please feel free to consult your submissions and/or official solutions as to how they have been applied to different datasets.  As always, if something appears to be unclear, please ask questions -- we may change to private mode those that in our opinion reveal too many details as we see fit.*

```{r setup, include=FALSE}
packages <- c("car", "corrplot", "DAAG", "GGally", "glmnet",
              "leaps", "mctest", "readxl", "tidyverse")
# Library loading (https://stackoverflow.com/questions/18931006/how-to-suppress-warning-messages-when-loading-a-library)
shhh <- suppressPackageStartupMessages # It's a library, so shhh!
# lapply(packages, install.packages)
invisible(shhh(lapply(packages, library, character.only = TRUE)))

# Prevent scientific notation: https://stackoverflow.com/a/25947542/8703244
options(scipen=999)

knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 10)
```

# Sub-problem 1: load and summarize the data (20 points)

*Download and read in the data, produce numerical and graphical summaries of the dataset attributes, decide whether they can be used for modeling in untransformed form or any transformations are justified, comment on correlation structure and whether some of the predictors suggest relationship with the outcome.*

>The inputs are as follows  
>X1=the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)  
>X2=the house age (unit: year)  
>X3=the distance to the nearest MRT station (unit: meter)  
>X4=the number of convenience stores in the living circle on foot (integer)  
>X5=the geographic coordinate, latitude. (unit: degree)  
>X6=the geographic coordinate, longitude. (unit: degree)  
>  
>The output is as follow  
>Y= house price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared)  

## Load data

Read in data, rename variables

``` {r load data}
# read data, remove index column, check for NAs
data <- read_excel("Real estate valuation data set.xlsx")
data <- subset(data, select = -No)
anyNA(data)

# use meaningful names
names(data) <- c("date", "age", "mrt.dist", "stores", "lat", "lon", "price")
```

## Data summary, review, and transformation

```{r data summary}
# Literal summary and correlations
summary(data)
#cor(data)

# Histograms to look at distribution
old.par <- par(mfrow = c(3, 3))
for (name in names(data)){
  hist(data[[name]], main = paste("Histogram of", name), xlab = name)
}
par(old.par)

# MRT distance is really skewed. See what log transform looks like
par(mfrow = c(1, 2))
hist(data$mrt.dist, main = "Histogram of MRT distance")
hist(log(data$mrt.dist), main = "Histogram of log-transformed MRT distance")
par(old.par)

# log is better distribution, but still not perfectly normal
# Will use log of distance going forward ("re" for real estate)
re <- subset(data, select = -mrt.dist)
re$log.mrt = log(data$mrt.dist)
re <- select(re, price, everything())
summary(re)

corrplot.mixed(cor(re), order = "hclust",upper = "color",
               number.cex = 1.5, tl.cex = 1, tl.col = "black")

# Check out plots
ggpairs(re, progress = FALSE)

# Zoom in on price
par(mfrow = c(2, 3))
plot(price ~ ., data = re, col = ifelse(price < 100, "black", "red"))
par(old.par)

# Seems to be an outlier with price near 120
# idea from https://stackoverflow.com/a/44089981/8703244
bp <- boxplot(re$price)
bp$out
which(re$price == max(bp$out))

# Remove row 271
re <- re[-(which(re$price == max(bp$out))),]
ggpairs(re, progress = FALSE)
```

## Discussion

+ Observations
  + No `NA`s in data -- pre-cleaned!
  + Distance to MRT is highly skewed
  + Some other variables not normally distributed
  + House price has an obvious outlier -- 117.5 vs next highest, 78.3 (median 38.45)
+ Correlations
  + Latitude, longitude, and number of stores moderately positively correlated with price (0.55, 0.52, 0.57, respectively)
  + Distance to MRT station _negatively_ correlated with price (-0.67 untransformed, -0.73 after log transform)
    + Wouldn't hold up in Boston or NYC!
    + Other analysis seems to show _no effect_ of [distance to MRT on housing prices in Taipei](https://www.tandfonline.com/doi/abs/10.1080/14445921.2016.1158938) (did the research because this seems unintuitive to me) 
+ Corrections
  + log transform on MRT distance (`log.mrt`)
  + Other variables left un-transformed for now.
    + Some clustering/clumping makes sense (longitude, age, date)
    + Data is distrubuted across measured values
    + NOTE: 
      + May be useful to transform longitude to an absolute value of difference from median or mean (or log thereof) -- could correct V shaped-data
      + Might be interesting to see what the effect of the sales date spike had on prices
  + Remove data point with price 117.5. Other 2 outliers left in for now

# Sub-problem 2: multiple linear regression model (25 points)

*Using function `lm` fit model of outcome as linear function of all predictors in the dataset. Present and discuss diagnostic plots. Report 99% confidence intervals for model parameters that are statistically significantly associated with the outcome and discuss directions of those associations. Obtain mean prediction (and corresponding 90% confidence interval) for a new observation with each attribute set to average of the observations in the dataset. Describe evidence for potential collinearity among predictors in the model.*

## `lm()` and diagnostics

```{r simple linear regression and diagnostics}
fit <- lm(price ~ ., data = re)
summary(fit)
par(mfrow = c(2, 2))
plot(fit)
par(old.par)
# Fancier Q-Q plot
qqPlot(fit)

# Graphics are nice, bur TEST for normailty of distrubution of residuals
shapiro.test(fit$residuals)


# Colinearirty question
# F-G test idea: https://datascienceplus.com/multicollinearity-in-r/
vif(fit)
omcdiag(subset(re, select = -price), re$price)
imcdiag(subset(re, select = -price), re$price)

# Double-check longitude
cor(re$lon, subset(re, select = -lon), method = "pearson")
cor(re$lon, subset(re, select = -lon), method = "spearman")

# Try model without longitude
fit1 <- lm(price ~ . -lon, data = re)
summary(fit1)
par(mfrow = c(2, 2))
plot(fit1)

# Double-check: Is log tansform of price useful?
fit2 <- lm(log(price) ~ ., data = re)
summary(fit2)
plot(fit2)

# Try a model with additional "bad" points removed
re.modified <- re[-c(114, 221, 312),]
fit3 <- lm(price ~ ., data = re.modified)
summary(fit3)
plot(fit3)

par(old.par)
```

## Confidence interval and prediction

```{r conf int and prediction}
# 99% Confidence interval for all coefficients in the fit
confint(fit, level = 0.99)

# Predicted value based on mean of all categories and 90% CI
newdata <- data.frame(date = mean(re$date),
                      age = mean(re$age),
                      stores = mean(re$stores),
                      lat = mean(re$lat),
                      lon = mean(re$lon),
                      log.mrt = mean(re$log.mrt))
predict(fit, newdata = newdata, interval = "prediction", level = 0.90)
mean(re$price)
```

## Discussion

Present and discuss diagnostic plots. 

+ A few points (114, 221, 312) are out of the expected range with standardized residual > 2, but not a worrying amount.
  + removing these points (`fit3`) does result in slightly increased adjusted $R^2$, but not enough to merit removal -- I'd err on the side of leaving data in unless it's really odd.
+ Residuals appear (roughly) evenly distributed.
  + Possibly a slight spread with increase in fitted values -- might be good to check out log transform of `price`
  + log transform results in worse (?) standardized residuals (`fit2`)
+ After trying modifications, I think the best strategy is to _not_ modify things further at this point and try other methods (variable selection, interaction or higher-order terms) to see if improvements can be found.

Report 99% confidence intervals for model parameters that are statistically significantly associated with the outcome and discuss directions of those associations. 

+ CIs for parameters above in confidence interval section
+ Positive coefficients
  + sale date
  + number of stores
  + latitude
+ Negative coefficients
  + house age
  + (log transformed) distance to MRT
  + (intercept)
+ Longitude's 99% CI includes 0, as expected

Obtain mean prediction (and corresponding 90% confidence interval) for a new observation with each attribute set to average of the observations in the dataset. 

+ predicted: 37.79 
+ 90% CI [25.78, 49.79]
+ As expected, predicted value == actual mean price
  + Prediction interval will also be most narrow at the mean and spread with distance from mean

Describe evidence for potential collinearity among predictors in the model.

+ No collinearity seen by VIF
+ [Multicollinearity testing](https://datascienceplus.com/multicollinearity-in-r/) suggests there may be an issue with `lon`gitude
+ While not _directly_ correlated with other data, some combination of data may recapitulate `lon`gitude data.
+ Model with `lon`gitude removed (`fit1`) not really different by adjusted $R^2$, so likely just not significant.
  + Makes sense as there isn't much spread in the data -- similar for all data


# Sub-problem 3: choose optimal models by exhaustive, forward and backward selection (20 points)

*Use `regsubsets` from library `leaps` to choose optimal set of variables for modeling real estate valuation and describe differences and similarities between attributes deemed most important by these approaches.*

## Generate models via multiple methods

Adapted from Homework 5

```{r stepwise model selection}
summary.metrics <- NULL
which.all <- list()

for (method in c("exhaustive", "backward", "forward", "seqrep")) {
  rs.result <- regsubsets(price ~ ., data = re, method = method, nvmax = ncol(re))
  rs.result.summ <- summary(rs.result)
  which.all[[method]] <- rs.result.summ$which
  for (metric in c("rsq", "rss", "adjr2", "cp", "bic")) {
    summary.metrics <- rbind(summary.metrics,
      data.frame(method = method, metric = metric,
                nvars = 1:length(rs.result.summ[[metric]]),
                value = rs.result.summ[[metric]]))
  }
}
```

## Visualize stepwise selection results

```{r visualize model info}
# Plot stats for models
ggplot(summary.metrics, aes(x = nvars, y = value, shape = method, color = method)) + 
  geom_path() + 
  geom_point() + 
  facet_wrap(~metric, scales = "free") +
  theme(legend.position="top") + 
  theme_bw() +
  scale_x_continuous(breaks = seq(2, ncol(re), 2))

# Look at Cp, for instance
#summary.metrics[which(summary.metrics$nvars >= 4 & summary.metrics$nvars <= 6 & summary.metrics$metric == "cp"), ]

# Make a data frame based on column info from model 5
models <-as.data.frame(which.all$exhaustive)[FALSE, ]
models$method = character()
# Get info on model 5
for (method in c("exhaustive", "backward", "forward", "seqrep")) {
  temp <- as.data.frame(method = method, which.all[[method]])[5, ]
  temp$method <- method
  models <- rbind(models, temp)
}
# Reorder data frame to put method first
models <- select(models, method, everything())
models

# Bigger picture of the models as above
old.par <- par(mfrow = c(2, 2), ps = 16, mar = c(5, 7, 2, 1))
for (method in names(which.all)) {
  image(1: nrow(which.all[[method]]),
        1: ncol(which.all[[method]]),
        which.all[[method]],
        xlab = "N(vars)",
        ylab = "",
        xaxt = "n",
        yaxt = "n",
        breaks = c(-0.5, 0.5, 1.5),
        col = c("white","darkgreen"), 
        main = method)
  axis(1, 1:nrow(which.all[[method]]), rownames(which.all[[method]]))
  axis(2, 1:ncol(which.all[[method]]), colnames(which.all[[method]]), las=2)
}
par(old.par)
```

## Discussion

+ By various metrics, a 5-variable model looks to be best by forward, backward, and stepwise selection
+ seqrep suggests all 6 variables, with 3 variables as the next best
  + `log.mrt`, the most highly-correlated variable, is not present in 4 or 5 variable model using this method. 
    + That doesn't seem to make sense given the high correlation
    + I'd be interested to look into out what's happening with this method, where it's useful, and why it has seemed off in our uses in class.
+ Interestingly, for all but seqrep:
  + number of stores doesn't show up until the 5-variable model although correlation with price is 2nd highest
  + House age is included relatively early compared to its modest correlation with price

# Sub-problem 4: optimal model by resampling (20 points)

*Use cross-validation or any other resampling strategy of your choice to estimate test error for models with different numbers of variables.  Compare and comment on the number of variables deemed optimal by resampling versus those selected by `regsubsets` in the previous task.*

## Cross-validation data

Again adapted from Homework 5

### Prediction function 

```{r predictRegsubsets}
predict.regsubsets <- function (object, newdata, num.vars, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = num.vars)
  xvars <- names(coefi)
  return(mat[, xvars] %*% coefi)
}
```

### Actual cross-validation (50-50 split)

```{r Cross validation, cache=TRUE}
var.select.cv <- NULL

whichSum <-
  array(0,
        dim = c(ncol(model.matrix(price ~ ., data = re)) - 1,
                ncol(model.matrix(price ~ ., data = re)),
                4),
        dimnames = list(
          NULL,
          colnames(model.matrix(price ~ ., data = re)),
          c("exhaustive", "backward", "forward", "seqrep")
        ))
# Split data into training and test 500 times:
nTries <- 500
for (i in 1:nTries) {
  bTrain <- sample(rep(c(TRUE, FALSE), length.out = nrow(re)))
  # Try each method available in regsubsets
  # to select the best model of each size:
  for (method in c("exhaustive", "backward", "forward", "seqrep")) {
    rsTrain <-
      regsubsets(price ~ .,
                 data = re[bTrain, ],
                 nvmax = ncol(model.matrix(price ~ ., data = re)) - 1,
                 method = method)
    # Add up variable selections:
    whichSum[, , method] <-
      whichSum[, , method] + summary(rsTrain)$which
    # Calculate test error for each set of variables
    # using predict.regsubsets implemented above:
    for (num.vars in 1:(ncol(model.matrix(price ~ ., data = re))-1)) {
      # make predictions:
      testPred <- predict(rsTrain, re[!bTrain, ], num.vars = num.vars)
      # calculate MSE (mean() function was giving errors, so calc'd manually:
      mseTest <- sum(((testPred - re[!bTrain, "price"])^2)) / length(testPred)
      # add to data.frame for future plotting:
      var.select.cv <-
        rbind(var.select.cv,
              data.frame(
                sim = i,
                sel = method,
                vars = num.vars,
                mse = c(mseTest, summary(rsTrain)$rss[num.vars] / sum(bTrain)),
                trainTest = c("test", "train")
              ))
    }
  }
}
```

## Look at error rates

Modify value of N for exploration

```{r error rates for different methods}
# Looking at test error from N-variable models:
N <- 5

# Forward and Backward commented out because they're the same as Exhaustive in this case

# Exhaustive
summary(var.select.cv$mse[var.select.cv$vars == N & 
                          var.select.cv$trainTest == "test" & 
                          var.select.cv$sel == "exhaustive"])
# Forward
#summary(var.select.cv$mse[var.select.cv$vars == N & var.select.cv$trainTest == "test" & var.select.cv$sel == "forward"])
# Backward
#summary(var.select.cv$mse[var.select.cv$vars == N & var.select.cv$trainTest == "test" & var.select.cv$sel == "backward"])
# SeqRep
summary(var.select.cv$mse[var.select.cv$vars == N & 
                          var.select.cv$trainTest == "test" & 
                          var.select.cv$sel == "seqrep"])

```

## CV plot

```{r Plot cross validation, fig.width=20}
# plot MSEs by training/test, number of 
# variables and selection method:
ggplot(var.select.cv, aes(x = factor(vars), y = mse, color = sel)) + 
  geom_boxplot() + 
  facet_wrap(~trainTest) +
  labs(title = "Cross-validation MSE by number of variables (500 replicates)", 
       x = "# variables") +
  theme_bw() +
  theme(text = element_text(size = 20))
```

## Final stepwise model

```{r Best stepwise model}
fit.step <- lm(price ~ . -lon, data = re)
summary(fit.step)
par(mfrow = c(2, 2))
plot(fit.step)
par(old.par)
```

## Discussion

Compare and comment on the number of variables deemed optimal by resampling versus those selected by `regsubsets` in the previous task

+ Result is (nearly) identical to that obtained by stepwise
  + 3 out of 4 methods suggest a 5-variable model is ideal -- adding a 6th doesn't give much if any improvement
  + seqrep results are much more variable

# Sub-problem 5: variable selection by lasso (15 points)

*Use regularized approach (i.e. lasso) to model property valuation.  Compare resulting models (in terms of number of variables and their effects) to those selected in the previous two tasks (by `regsubsets` and resampling), comment on differences and similarities among them.*

## Lasso regression

Still more adapted from HW 5!

```{r lasso regression}
# -1 to get rid of intercept that glmnet knows to include:
x <- model.matrix(price ~ ., data = re)[,-1]
head(x)  # To double-check
y <- re$price

lasso.res <- glmnet(x, y, alpha = 1)

# Look at lambda range used for comparison with cv.glmnet()
range(lasso.res$lambda)  # ~ 0 - 10
log10(range(lasso.res$lambda))

# NOTE: plotted with x = log(lambda) for easier comparison
plot(lasso.res, xvar = "lambda", label = TRUE)

```

## Cross-validation with `cv.glmnet()`

```{r lasso cross validation}
# Interesting how number of fold for cross validation changes the results
# Standard
cv.lasso.res <- cv.glmnet(x, y, alpha = 1, 
                          lambda = 10 ^ seq(-2, 2, 0.025), 
                          nfolds = 10)

# Leave-one-out
cv.lasso.res.loo <- cv.glmnet(x, y, alpha = 1, 
                          lambda = 10 ^ seq(-2, 2, 0.025), 
                          nfolds = nrow(re))
# Plots
plot(cv.lasso.res, main = "10-fold cross-validation")
plot(cv.lasso.res.loo, main = "Leave-one-out cross-validation")

# coef() gets the coefficients directly instead of predicting from the earlier model
# Result is (almost) identical and seems cleaner
coef(cv.lasso.res, s = "lambda.1se")
#predict(lasso.res, type = "coefficients", s = cv.lasso.res$lambda.1se)
coef(cv.lasso.res, s = "lambda.min")
#predict(lasso.res, type = "coefficients", s = cv.lasso.res$lambda.min)

coef(cv.lasso.res.loo, s = "lambda.1se")
coef(cv.lasso.res.loo, s = "lambda.min")
```

## Head-to-head with stepwise

```{r lasso vs step}
compare <- merge(
  data.frame(stepwise = fit.step$coefficients),
  data.frame(lasso = coef(cv.lasso.res, s = "lambda.1se")[, 1]),
  by = 0, all = TRUE
)
compare[is.na(compare)] <- 0
compare
```

## Discussion

+ Lasso regression (usually) results in 5 variables being included
  + Occasionally, `lon`gitude is not dropped, especially in min model
  + Model at 1 SE always has **much** lower weight on `lon`gitude if included
  + Various folds of cross-validation result in similar coefficients
+ Comparison to best stepwise model
  + All coefficients are the same sign
  + Lasso coefficients are smaller in magnitude (as expected) due to penalization for multiple variables

# Extra points problem: using higher order terms (10 points)

*Evaluate the impact of adding non-linear terms to the model.  Describe which terms, if any, warrant addition to the model and what is the evidence supporting their inclusion.  Evaluate, present and discuss the effect of their incorporation on model coefficients and test error estimated by resampling.*

## Second-order and interaction terms function

NOTE: left this in the RMD file, but I'm not using it in this version
```{r Quadratic and interaction terms, eval=FALSE, include=FALSE}
# Takes in a data frame and the outcome variable (so it can be removed)
interact <- function (df, outcome.var){
  # Get names of predictors
  predictors = names(select(df, -outcome.var))
  
  # Loop over each predictor
  for (i in 1:length(predictors)){
    # Loop over all predictors from self onward
    for (j in i:length(predictors)){
      # Store new column name
      newcol <- paste0(predictors[i], "X", predictors[j])
      # Calculate new data as product of 2 columns
      # and add new data to existing data frame
      df[newcol] = df[[predictors[i]]] * df[[predictors[j]]]
    }
  }
  return(df)
}
```

## Fit model with tons of terms

``` {r ALL THE TERMS}
# Add quadratic and interaction terms
# https://stackoverflow.com/questions/27067429/rfit-dynamic-number-of-explanatory-variable-into-polynomial-regression
form.inter.quad <- formula(paste(names(re)[1], " ~ . * . +",
                           paste0("I(", names(re)[-1], "^ 2)", collapse="+")))

fit.inter.quad <- lm(form.inter.quad, data = re)
summary(fit.inter.quad)
par(mfrow = c(2, 2))
plot(fit.inter.quad)
par(old.par)

#plot(re$price, predict(fit.inter.quad))
#abline(a = 0, b = 1)

# This is a mess
vif(fit.inter.quad)
```

## Reduce number of terms via stepwise selection

```{r Stepwise on all the extra terms}
# Just running once due to computational cost
# using BIC (log(n)) to minimize number of variables
fit.inter.quad.step <- step(fit.inter.quad, direction = "both", trace = FALSE, k = log(nrow(re)))
summary(fit.inter.quad.step)
```

```{r Model comparison via resampling, cache=TRUE}
# Formulas to use in cross-validation
formulas <- c(fit$call$formula, 
             fit.step$call$formula, 
             #fit.inter.quad$call$formula, # Doesn't work for this one!
             form.inter.quad,
             fit.inter.quad.step$call$formula)
model.names <- c("full", "step", "high.order", "high.order.step")

# Space to store MSE
mse <- NULL

for (n in 1:500) {
  # 80/20 split for train/test
  train <- sample(1:nrow(re), size = round(0.8 * nrow(re)))
  # For each formula ...
  for (i in seq_along(formulas)) {
    # Build model on training, store error
    model <- lm(formulas[[i]], data = re[train, ])
    mse <- rbind(mse, c(mean(model$residuals ^ 2),
                        model.names[i], 
                        "train"))
    
    # Predict test data and calculate error
    # Suppress warnings from trying to predict on messy model
    mse <- rbind(mse, c(suppressWarnings(mean((predict(model, re[-train, ]) - re$price[-train]) ^ 2)),
                        model.names[i], 
                        "test"))
  }
}
# Convert to dataframe for plotting
mse <- data.frame(error = as.numeric(mse[, 1]),
                  model = mse[, 2],
                  train.test = mse[, 3])

ggplot(mse, aes(x = factor(model, levels = model.names), y = error, color = model)) + 
  geom_boxplot() + 
  facet_wrap(~factor(train.test, levels = c("train", "test"))) +
  labs(title = "Cross-validation MSE per model (500 replicates)", 
       x = "model") +
  theme_bw() +
  theme(text = element_text(size = 20),
        axis.text.x=element_text(angle=45, hjust=1))
```

## Discussion

+ Adding lots of variables with no good reason is not helpful
  + Massive multicollinearity as seen by VIF
    + Use of `poly` instead of `I()` may be better as it should create orthogonal coefficients. In practice, it didn't help
    + Lots of "singularities" -- so correlated that design matrix can't be inverted
+ Stepwise (backward-forward for simplicity) removes most of the variables
  + Linear
    + Sale date
    + House age
    + Number of nearby stores
    + Latitude
    + (log) distance to MRT
  + Quadratic
    + House age
    + Latitude
    + (log) distance to MRT
  + Interaction
    + Number of nearby stores:Latitude
    + Number of nearby stores:(log) distance to MRT
    + (log) distance to MRT:Latitude
+ While the massive model performs the best on training data (as expected), it overfits the data and doesn't perform as well on test data.
  + Surprisingly, the amount of overfitting isn't as bad as I'd expected.
+ The stepwise model generated from the massive model performs well on training data and the best on test data.

    