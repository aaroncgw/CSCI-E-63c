---
title: 'CSCI E-63C: Week 8 Problem Set'
author: 'Joshua Sacher'
date: '`r Sys.Date()`'

output:
  html_document:
    df_print: kable
    number_sections: true
    toc: true
    toc_float:
      collapsed: false
---

```{r setup, include=FALSE}
library(ape)
library(cluster)
library(dendextend)
library(data.table)
library(ggplot2)
library(ISLR)
library(MASS)
knitr::opts_chunk$set(echo = TRUE, fig.width = 15, fig.height = 10)
options(width = 200)
```

# Preface

In this problem set we will exercise some of the unsupervised learning approaches on [2018 Global Health Observatory (GHO) data](https://www.who.int/gho/publications/world_health_statistics/2018/en/).  It is available at that website in the form of [Excel file](https://www.who.int/gho/publications/world_health_statistics/2018/whs2018_AnnexB.xls?ua=1), but its cleaned up version ready for import into R for further analyses is available at CSCI E-63C canvas course web site [whs2018_AnnexB-subset-wo-NAs.txt](https://canvas.harvard.edu/files/8709281/download?download_frd=1).  The cleaning and reformatting included: merging data from the three parts of Annex B, reducing column headers to one line with short tags, removal of ">", "<" and whitespaces, conversion to numeric format, removal of the attributes with more than 20% of missing values and imputing the remaining missing values to their respective medians.  You are advised to save yourself that trouble and start from preformatted text file available at the course website as shown above.  The explicit mapping of variable names to their full description as provided in the original file is available in Excel file [whs2018_AnnexB-subset-wo-NAs-columns.xls](https://canvas.harvard.edu/files/8709280/download?download_frd=1) also available on the course canvas page.  Lastly, you are advised to download a local copy of this text file to your computer and access it there (as opposed to relying on R ability to establish URL connection to canvas that potentially requires login etc.)

Short example of code shown below illustrates reading this data from a local copy on your computer (assuming it has been copied into current working directory of your R session -- `getwd()` and `setwd()` commands are helpful to find out what is it currently and change it to desired location) and displaying summaries and pairs plot of five (out of almost 40) arbitrary chosen variables.  This is done for illustration purposes only -- the problems in this set expect use of all variables in this dataset.

```{r WHS,fig.height=10,fig.width=10}
whs <- read.table("whs2018_AnnexB-subset-wo-NAs.txt",sep="\t",header=TRUE,quote="")
summary(whs[,c(1,4,10,17,26)])
pairs(whs[,c(1,4,10,17,26)])
```

In a way this dataset is somewhat similar to the `USArrests` dataset extensively used in ISLR labs and exercises -- it collects various continuous statistics characterizing human population across different territories.  It is several folds larger though -- instead of `r nrow(USArrests)` US states and `r ncol(USArrests)` attributes in `USArrests`, world health statistics (WHS) data characterizes `r nrow(whs)` WHO member states by `r ncol(whs)` variables.  Have fun!

The following problems are largely modeled after labs and exercises from Chapter 10 ISLR.  If anything presents a challenge, besides asking questions on piazza (that is always a good idea!), you are also encouraged to review corresponding lab sections in ISLR Chapter 10.

# Problem 1: Principal components analysis (PCA) (25 points)

The goal here is to appreciate the impact of scaling of the input variables on the result of the principal components analysis.  To that end, you will first survey means and variances of the attributes in this dataset (sub-problem 1a) and then obtain and explore results of PCA performed on data as is and after centering and scaling each attribute to zero mean and standard deviation of one (sub-problem 1b).

## Sub-problem 1a: means and variances of WHS attributes (5 points)

Compare means and variances of the *untransformed* attributes in the world health statisics dataset -- plot of variance vs. mean is probably the best given the number of attributes in the dataset.  Function `apply` allows to apply desired function (e.g. `mean` or `var` or `sd`) to each row or column in the table.  Do you see all `r ncol(whs)` attributes in the plot, or at least most of them?  (Remember that you can use `plot(inpX,inpY,log="xy")` to use log-scale on both horizontal and vertical axes.)  Is there a dependency between attributes' averages and variances? What is the range of means and variances when calculated on untransformed data?  Which are the top two attributes with the highest mean or variance?  What are the implications for PCA rendition of this dataset (in two dimensions) if applied to untransformed data?

### Code

```{r}
# Calculate mean and variance per column
avgs <- lapply(whs, mean)
vari <- lapply(whs, var)

# Just for fun:
plot(avgs, vari, main = "WHS data variance vs. mean", xlab = "mean", ylab = "variance")

# Better scale:
plot(avgs, vari, log = "xy", 
     main = "WHS data variance vs. mean (log scale)", 
     xlab = "mean", ylab = "variance")

range(avgs)
range(avgs)[2] - range(avgs)[1]

range(vari)
range(vari)[2] - range(vari)[1]

# https://stackoverflow.com/questions/27312311/sort-a-named-list-in-r
avgs <- avgs[order(unlist(avgs),decreasing=TRUE)]
vari <- vari[order(unlist(vari),decreasing=TRUE)]

avgs[1:2]
vari[1:2]
```

### Discussion

Is there a dependency between attributes' averages and variances? 

* Yes
  * Positively correlated
  * (Roughly) linearly correlated

What is the range of means and variances when calculated on untransformed data?  

* Mean: (0.195, 7,732,494.608)
  * Difference 7,732,494
  * ~7 orders of magnitute
* Variance: (0.131, 1,287,990,390,309,082.5)
  * Difference 1,287,990,390,309,082
  * ~15 orders of magnitute (!!)

Which are the top two attributes with the highest mean or variance?  

* `NTDinterventions`
  * Mean: 7,732,495
  * Variance: 1,287,990,390,309,082
* `TotalPopulation`
  * Mean: 38,300
  * Variance: 20,543,303,684

What are the implications for PCA rendition of this dataset (in two dimensions) if applied to untransformed data?

* The first 2 principle components would be wholly (or nearly wholly) composed of NTDinterventions and TotalPopulation, as these explain the *vast* majority of the variance
  * Next highest variance is ~10,000 times smaller

## Sub-problem 1b: PCA on untransformed and scaled WHS data (20 points)

Perform the steps outlined below *both* using *untransformed* data and *scaled* attributes in WHS dataset (remember, you can use R function `prcomp` to run PCA and to scale data you can either use as input to `prcomp` the output of `scale` as applied to the WHS data matrix or call `prcomp` with parameter `scale` set to `TRUE`). To make it explicit, the comparisons outlined below have to be performed first on the unstransformed WHS data and then again on scaled WHS data -- you should obtain two sets of results that you could compare and contrast.

### Perform PCA

### 1. Obtain results of principal components analysis of the data (by using `prcomp`)

```{r PCA}
pca.untrans <- prcomp(whs, scale. = FALSE)
pca.trans <- prcomp(whs, scale. = TRUE)

```


### 2. Generate scree plot of PCA results (by calling `plot` on the result of `prcomp`)

```{r PCA plots}
old.par <- par(mfrow = c(1, 2))
plot(pca.untrans, main = "Variance of Untransformed Components")
plot(pca.trans, main = "Variance of Transformed Components")
par(old.par)
```


### 3. Generate plot of the two first principal components using `biplot`.  

```{r biplot}
suppressWarnings(biplot(pca.untrans, scale = 0))
biplot(pca.trans, scale = 0)

```

#### Discussion

Which variables seem to predominantly drive the results of PCA when applied to untransformed data?   

* `NTDinterventions` is overwhelmingly the first principal component
* `TotalPopulation` is overwhelmingly the second prinicpal component
  * Almost perfectly orthogonal to `NTDinterventions`

Please note that in case of untransformed data you should expect `biplot` to generate substantial number of warnings.  Usually in R we should pay attention to these and understand whether they indicate that something went wrong in our analyses.  In this particular case they are expected -- why do you think that is?

* The error `zero-length arrow is of indeterminate angle and so skipped` is expected as most variables don't contribute to PC1 or PC2
* Hooray for `suppressWarnings()`

### 4. The field `rotation` in the output of `prcomp` contains *loadings* of the 1st, 2nd, etc. principal components (PCs) -- that can interpreted as contributions of each of the attributes in the input data to each of the PCs.

```{r rotation}
# Untransformed
pca.untrans$rotation[, 1:5]

# Transformed
pca.trans$rotation[, 1:5]
```

#### Discussion

+ What attributes have the largest (by their absolute value) loadings for the first and second principal component?
  * `NTDinterventions` is ~99.999% of PC1
  * `TotalPopulation` is ~99.999% of PC2
+ How does it compare to what you have observed when comparing means and variances of all attributes in the world health statistics dataset?
  * Direct match for the largest means/variances
  
### 5. Calculate percentage of variance explained (PVE) by the first five principal components (PCs).  You can find an example of doing this in ISLR Chapter 10.4 (Lab 1 on PCA).

```{r PVE}
# Untransformed
pca.untrans.var <- pca.untrans$sdev ^ 2
pve.untrans <- pca.untrans.var / sum(pca.untrans.var)
pve.untrans[1:5]
sum(pve.untrans[1:5])

plot(pve.untrans, main = "Variance per PC (black) and cumulative total (red)", 
     ylim = c(0,1), xlab = "PC", ylab = "Variance explained", type = "b")
par(new = TRUE)
plot(cumsum(pve.untrans), col = "red", type = "b", xlab = "", ylab = "", ylim = c(0,1))
old.par


# Transformed
pca.trans.var <- pca.trans$sdev ^ 2
pve.trans <- pca.trans.var / sum(pca.trans.var)
pve.trans[1:5]
sum(pve.trans[1:5])

plot(pve.trans, main = "Variance per PC (black) and cumulative total (red)",
     ylim = c(0,1),  xlab = "PC", ylab = "Variance explained", type = "b")
par(new = TRUE)
plot(cumsum(pve.trans), col = "red", type = "b", xlab = "", ylab = "", ylim = c(0,1))
old.par

```

### Discussion

Now that you have PCA results when applied to untransformed and scaled WHS data, please comment on how do they compare and what is the effect of scaling?  

* Untransformed ends up with many components composed of (roughly) a single variable
  * Basically a costly way of plotting each variable vs each other based on variance

What dataset attributes contribute the most (by absolute value) to the top two principal components in each case (untransformed and scaled data)?  What are the signs of those contributions?  How do you interpret that?

* Untransformed
  * `NTDinterventions` is nearly all the variance
  * `TotalPopulation` cleans up most of what's left over
  * Both are positive
* Transformed
  * `LifeExpectancyF` is highest in PC1, followed closely by other health-related variables'
    This variable has the highest scaled variance
  * `CHEperCapita` is highest in PC2
  * Both are negative -- negatively correlate with other variables
  * Many variables are close together in terms of magnitude

### More PCA plots

Please note, that the output of `biplot` with almost 200 text labels on it can be pretty busy and tough to read.  You can achieve better control when plotting PCA results if instead you plot the first two columns of the `x` attribute in the output of `prcomp` -- e.g. `plot(prcomp(USArrests,scale=T)$x[,1:2])`.  Then given this plot you can label a subset of countries on the plot by using `text` function in R to add labels at specified positions on the plot.  Please feel free to choose several countries of your preference and discuss the results.  Alternatively, indicate US, UK, China, India, Mexico, Australia, Israel, Italy, Ireland and Sweden and discuss the results.  

```{r More plots}
countries <- c("Australia", "China", "Haiti", "India", "Ireland", "Israel", 
               "Indonesia", "Italy", "Japan", "Mexico", "Nigeria", "Russia", 
               "Sierra Leone", "Sweden", "United Kingdom", "United States of America")

# Untransformed PCA
untrans.countries.x <- subset(pca.untrans$x, row.names(pca.untrans$x) %in% countries)
plot(pca.untrans$x[, 1:2], main = "PC1 and PC2 of Untransformed Data")
text(untrans.countries.x[, 1:2], 
     labels = row.names(untrans.countries.x), 
     pos = 4)

# Original frame of reference (...for reference)
whs.countries <- subset(whs, row.names(whs) %in% countries)
plot(whs$NTDinterventions, whs$TotalPopulation, 
     main = "Total Population vs Neglected Tropical Disease Interventions \n (for comparison to PCA plot)")
text(whs.countries$NTDinterventions, 
     whs.countries$TotalPopulation, 
     row.names(whs.countries), pos = 4)

# Transformed PCA
trans.countries.x <- subset(pca.trans$x, row.names(pca.trans$x) %in% countries)
plot(pca.trans$x[, 1:2], main = "PC1 and PC2 of Transformed Data")
text(trans.countries.x[, 1:2], 
     labels = row.names(trans.countries.x), 
     pos = 4)

```

#### Discussion

Where do the countries you have plotted fall in the graph?  Considering what you found out about contributions of different attributes to the first two PCs, what do their positions tell us about their (dis-)similarities in terms of associated health statistics?
* India is very high on PC1, which indicates a high rate of **N**eglected **T**ropical **D**isease interventions
  * Nigeria has 2nd highest rate
  * Western countries have a very low rate, as expected
* China has a massive population, so is very high in PC2
  * Due to rotation, India isn't as high in PC2 as it would be in a direct plot of `TotalPopulation` vs `NTDinterventions`
* In the transformed PCA, the countries separate out much better
  * PC1 looks to (mostly) be negatively related to life expectancy metrics
  * PC2 is (mostly) negatively related to current health expenditure metrics

# Problem 2: K-means clustering (20 points)

The goal of this problem is to practice use of K-means clustering and in the process appreciate the variability of the results due to different random starting assignments of observations to clusters and the effect of parameter `nstart` in alleviating it.

## Sub-problem 2a: k-means clusters of different size (5 points)

Using function `kmeans` perform K-means clustering on *explicitly scaled* (e.g. `kmeans(scale(x),2)`) WHS data for 2, 3 and 4 clusters.  Use `cluster` attribute in the output of `kmeans` to indicate cluster membership by color and/or shape of the corresponding symbols in the plot of the first two principal components generated independently on the same (scaled WHS) data.  E.g. `plot(prcomp(xyz)$x[,1:2],col=kmeans(xyz,4)$cluster)` where `xyz` is input data.  

### Code

```{r K-means k values}
km2 <- kmeans(scale(whs), centers = 2, nstart = 100)
km3 <- kmeans(scale(whs), centers = 3, nstart = 100)
km4 <- kmeans(scale(whs), centers = 4, nstart = 100)

plot(pca.trans$x[, 1:2], col=km2$cluster,
     main = "PC1 and PC2 of Transformed Data \n colored by cluster (K = 2)")
text(trans.countries.x[, 1:2], 
     labels = row.names(trans.countries.x), 
     pos = 4)

plot(pca.trans$x[, 1:2], col=km3$cluster,
     main = "PC1 and PC2 of Transformed Data \n colored by cluster (K = 3)")
text(trans.countries.x[, 1:2], 
     labels = row.names(trans.countries.x), 
     pos = 4)

plot(pca.trans$x[, 1:2], col=km4$cluster,
     main = "PC1 and PC2 of Transformed Data \n colored by cluster (K = 4)")
text(trans.countries.x[, 1:2], 
     labels = row.names(trans.countries.x), 
     pos = 4)
```

### Discussion

Describe the results.  Which countries are clustered together for each of these choices of $K$?

* For $K = 2$ and $K = 3$, the countries are split roughly into "bins" along PC1 from above
* For $K = 4$, the additional group is composed of India and China, suggesting population is affecting the clustering.

## Sub-problem 2b: variability of k-means clustering and effect of `nstart` parameter (15 points)

By default, k-means clustering uses random set of centers as initial guesses of cluster centers.  Here we will explore variability of k-means cluster membership across several such initial random guesses.  To make such choices of random centers reproducible, we will use function `set.seed` to reset random number generator (RNG) used in R to make those initial guesses to known/controlled initial state.

Using the approach defined above, repeat k-means clustering of *explicitly scaled* WHS data with four (`centers=4`) clusters three times resetting RNG each time with `set.seed` using seeds of 1, 2 and 3 respectively (and default value of `nstart=1`).  Indicate cluster membership in each of these three trials on the plot of the first two principal components using color and/or shape as described above.  

### Code

```{r}
# For nstart values 1 and 100
for (n in c(1, 100)){
  # Set seed to 1, 2, 3
  for (seed in 1:3){
    set.seed(seed)
    # Append kmeans results to vector
    k <- kmeans(scale(whs), centers = 4, nstart = n)
    
    plot(pca.trans$x[, 1:2], col=k$cluster,
         main = paste("Seed:", seed, "nstart:", n))
    text(trans.countries.x[, 1:2], 
         labels = row.names(trans.countries.x), 
         pos = 4)
    
    # Use cat() to evaluate "\n" as a newline
    cat(paste("Seed:", seed, "\nnstart:", n, 
              "\nwithin ss:", k$tot.withinss,
              "\nbetween ss:", k$betweenss, 
              "\nratio:", (k$tot.withinss / k$betweenss), 
              "\n\n"))
  }
}

```

### Discussion

Two fields in the output of `kmeans` -- `tot.withinss` and `betweenss` -- characterize within and between clusters sum-of-squares.  Tighter clustering results are those which have smaller ratio of within to between sum-of-squares.  

What are the resulting ratios of within to between sum-of-squares for each of these three k-means clustering results (with random seeds of 1, 2 and 3)?

* All random seeds converge with `nstarts = 100`.
* Only seed 3 matches the performance of 100 starts when `nstarts = 1` is used

| Seed\\nstarts |   1   |  100  |
|:--------------:|:-----:|:-----:|
| 1 | 1.044 | 1.041 |
| 2 | 1.119 | 1.041 |
| 3 | 1.041 | 1.041 |

Please bear in mind that the actual cluster identity is assigned randomly and does not matter -- i.e. if cluster 1 from the first run of `kmeans` (with random seed of 1) and cluster 4 from the run with the random seed of 2 contain the same observations (country/states in case of WHS dataset), they are *the same* clusters.

Repeat the same procedure (k-means with four clusters for RNG seeds of 1, 2 and 3) now using `nstart=100` as a parameter in the call to `kmeans`.  Represent results graphically as before.  

* See above

How does cluster membership compare between those three runs now?  What is the ratio of within to between sum-of-squares in each of these three cases?  

* Cluster membership is identical when `nstarts` is 100.
* Ratio is 1.041 in all cases

What is the impact of using higher than 1 (default) value of `nstart`?  What is the ISLR recommendation on this offered in Ch. 10.5.1?

* `nstart` generates `n` selections of centroids, choosing the best one [Source](https://www.r-statistics.com/2013/08/k-means-clustering-from-r-in-action/)
* ISLR recommends using a "large value [...] such as 20 or 50" to avoid finding a local instead of global minimum

One way to achieve everything this sub-problem calls for is to loop over `nstart` values of 1 and 100, for each value of `nstart`, loop over RNG seeds of 1, 2 and 3, for each value of RNG seed, reset RNG, call `kmeans` and plot results for each combination of `nstart` and RNG seed value.

* Check!

# Problem 3: Hierarchical clustering (15 points)

## Sub-problem 3a: hierachical clustering by different linkages (10 points)

Cluster country states in (scaled) world health statistics data using default (Euclidean) distance and "complete", "average", "single" and "ward" linkages in the call to `hclust`.  Plot each clustering hierarchy, describe the differences.  For comparison, plot results of clustering *untransformed* WHS data using default parameters (Euclidean distance, "complete" linkage) -- discuss the impact of the scaling on the outcome of hierarchical clustering.

### Code

NOTE: Tried out some other plotting methods. For this many countries, I (personally) like the "fan" plot

```{r hierarchical clustering, fig.height=15}
# Some different visualization methods for hierarchical clustering:
# http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning

# Complete
hc.complete <- hclust(dist(scale(whs)), method = "complete")
# Vanilla R plot
plot(hc.complete,
     main = "Complete Linkage", 
     xlab = "", sub = "", cex = 0.7)
# rect.hclust(hc.complete, 4)

# ape library's phylo
plot(as.phylo(hc.complete),
     main = "Complete Linkage",
     label.offset = 0.2,
     cex = 0.5)

# Circular version!
clust <- km4$cluster
colors = c("darkred", "darkblue", "darkgreen", "black")
plot(as.phylo(hc.complete), type = "fan",
     main = "Complete Linkage Colored by K-Means Cluster",
     tip.color = colors[clust],
     cex = 0.8, label.offset = 0.25)

# Average
hc.average <- hclust(dist(scale(whs)), method = "average")
plot(hc.average, 
     main = "Average Linkage", 
     xlab = "", sub = "", cex = 0.75)
# rect.hclust(hc.average, 4)

# Single
hc.single <- hclust(dist(scale(whs)), method = "single")
plot(hc.single, 
     main = "Single Linkage", 
     xlab = "", sub = "", cex = 0.7)
# rect.hclust(hc.single, 4)

# Ward
hc.ward <- hclust(dist(scale(whs)), method = "ward.D2")
plot(hc.ward, 
     main = "Ward Linkage", 
     xlab = "", sub = "", cex = 0.7)
# rect.hclust(hc.ward, 4)

# Unscaled complete
plot(hclust(dist(whs), method = "complete"),
     main = "Unscaled Complete Linkage", 
     xlab = "", sub = "", cex = 0.7)
# rect.hclust(hclust(dist(whs)), 4)
```

### Discussion

* Differences
  * With complete linkage, heights are broadly distributed.
  * Average and single both give "skewed," "cascading" trees
  * Ward linkage gives more evenly distributed levels.
  * Ward Results in clusters with more equal membership, while others have some clusters with small numbers of countries (or even just one).
* Unscaled
  * Heights are nearly uniform except for a tiny number of countries -- no real differentiation

## Sub-problem 3b: compare k-means and hierarchical clustering (5 points)

Using function `cutree` on the output of `hclust` determine assignment of the countries in WHS dataset into top four clusters when using Euclidean distance and Ward linkage. (Feel free to choose which one of the two varieties of Ward linkage available in `hclust` you want to use here!).  Use function `table` to compare membership of these clusters to those produced by k-means clustering with four clusters in the Problem 2(b) when using `nstart=100` (and any of the RNG seeds) above.  Discuss the results.

### Code

```{r cutree, fig.height=15}
# Cut into 4 clusters
hc.cut <- cutree(hc.ward, 4)
clust <- km4$cluster
addmargins(table(clust, hc.cut))


# Tree plot colored by k-means membership
colors = c("darkred", "darkblue", "darkgreen", "black")
plot(as.phylo(hc.ward), direction = "downward",
     tip.color = colors[clust],
     cex = 0.5)
rect.hclust(hc.ward, 4)
```

### Discussion

* There is high concordance between the tree with 4 groups and K-means clustering with $K = 4$
  * Perfect agreement in 2 out of 4 clusters
  * 1 tree cluster with good agreement
  * 1 tree cluster composed of parts of 3 differend k-means cluster
